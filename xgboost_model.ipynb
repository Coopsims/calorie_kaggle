{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Calories Prediction - XGBoost Model\n",
    "\n",
    "This notebook implements an XGBoost model with hyperparameter tuning to predict calories burned during workouts for the Kaggle Playground Series competition.\n"
   ],
   "id": "83b1482ff5886de9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Import Libraries\n",
   "id": "6df12c7762bdfeb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error, r2_score\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n"
   ],
   "id": "6b2adbd380d5ae9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Load and Explore Data\n",
   "id": "eebe5a6a0c9e7c4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the training and test data\n",
    "train_data = pd.read_csv('playground-series-s5e5/train.csv')\n",
    "test_data = pd.read_csv('playground-series-s5e5/test.csv')\n",
    "\n",
    "# Display basic information about the training data\n",
    "print(\"Training data shape:\", train_data.shape)\n",
    "train_data.head()\n"
   ],
   "id": "ba5c200f2a0955f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in training data:\")\n",
    "print(train_data.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in test data:\")\n",
    "print(test_data.isnull().sum())\n"
   ],
   "id": "35bf716247ca0731"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Statistical summary of the training data\n",
    "train_data.describe()\n"
   ],
   "id": "aee95738448c66ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Exploratory Data Analysis\n",
   "id": "3a4d711c6a41d949"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Distribution of the target variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train_data['Calories'], kde=True)\n",
    "plt.title('Distribution of Calories')\n",
    "plt.xlabel('Calories')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Check if log transformation would make the distribution more normal\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(np.log1p(train_data['Calories']), kde=True)\n",
    "plt.title('Distribution of Log(Calories)')\n",
    "plt.xlabel('Log(Calories)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ],
   "id": "b21db07877741898"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "numeric_cols = train_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "correlation = train_data[numeric_cols].corr()\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ],
   "id": "65eb0b42130ab721"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Relationship between features and target\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "features = ['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp']\n",
    "for i, feature in enumerate(features):\n",
    "    sns.scatterplot(x=feature, y='Calories', data=train_data, ax=axes[i])\n",
    "    axes[i].set_title(f'{feature} vs Calories')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "69b87a7fd0f39670"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Feature Engineering\n",
   "id": "5bbc3a90a885d652"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a copy of the datasets for feature engineering\n",
    "train_fe = train_data.copy()\n",
    "test_fe = test_data.copy()\n",
    "\n",
    "# Calculate BMI (Body Mass Index)\n",
    "train_fe['BMI'] = train_fe['Weight'] / ((train_fe['Height'] / 100) ** 2)\n",
    "test_fe['BMI'] = test_fe['Weight'] / ((test_fe['Height'] / 100) ** 2)\n",
    "\n",
    "# Create interaction features\n",
    "train_fe['Duration_HeartRate'] = train_fe['Duration'] * train_fe['Heart_Rate']\n",
    "test_fe['Duration_HeartRate'] = test_fe['Duration'] * test_fe['Heart_Rate']\n",
    "\n",
    "train_fe['Weight_Duration'] = train_fe['Weight'] * train_fe['Duration']\n",
    "test_fe['Weight_Duration'] = test_fe['Weight'] * test_fe['Duration']\n",
    "\n",
    "# Add Body_Temp squared term to model curvature\n",
    "body_temp_mean = train_fe['Body_Temp'].mean()\n",
    "train_fe['Body_Temp_Squared'] = (train_fe['Body_Temp'] - body_temp_mean) ** 2\n",
    "test_fe['Body_Temp_Squared'] = (test_fe['Body_Temp'] - body_temp_mean) ** 2\n",
    "\n",
    "# Create age buckets to capture non-linear age effects\n",
    "# Define age bins\n",
    "age_bins = [0, 25, 35, 45, 55, 100]\n",
    "age_labels = ['<25', '25-35', '35-45', '45-55', '55+']\n",
    "\n",
    "# Create age buckets\n",
    "train_fe['Age_Bucket'] = pd.cut(train_fe['Age'], bins=age_bins, labels=age_labels)\n",
    "test_fe['Age_Bucket'] = pd.cut(test_fe['Age'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# One-hot encode age buckets\n",
    "age_dummies_train = pd.get_dummies(train_fe['Age_Bucket'], prefix='Age')\n",
    "age_dummies_test = pd.get_dummies(test_fe['Age_Bucket'], prefix='Age')\n",
    "\n",
    "# Add age dummy columns to the dataframes\n",
    "train_fe = pd.concat([train_fe, age_dummies_train], axis=1)\n",
    "test_fe = pd.concat([test_fe, age_dummies_test], axis=1)\n",
    "\n",
    "# Convert Sex to numerical (0 for female, 1 for male)\n",
    "train_fe['Sex_num'] = train_fe['Sex'].map({'female': 0, 'male': 1})\n",
    "test_fe['Sex_num'] = test_fe['Sex'].map({'female': 0, 'male': 1})\n",
    "\n",
    "# Display the new features\n",
    "train_fe.head()\n"
   ],
   "id": "2e6398b812dafa7e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Prepare Data for Modeling\n",
   "id": "5fa961c335b9385d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define features and target\n",
    "features = ['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp', 'Sex_num', 'BMI', \n",
    "           'Duration_HeartRate', 'Weight_Duration', 'Body_Temp_Squared',\n",
    "           'Age_<25', 'Age_25-35', 'Age_35-45', 'Age_45-55', 'Age_55+']\n",
    "X = train_fe[features]\n",
    "y = train_fe['Calories']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n"
   ],
   "id": "4c98c543ebe31c0e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Train a Basic XGBoost Model\n",
   "id": "3ba0ae129810fea9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define a function to calculate RMSLE\n",
    "def rmsle(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "# Create a custom RMSLE evaluation metric for XGBoost\n",
    "def rmsle_xgb(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    y_pred = np.maximum(y_pred, 0)  # Ensure predictions are positive\n",
    "    return 'RMSLE', np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "# Train a basic XGBoost model\n",
    "basic_xgb = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "basic_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = basic_xgb.predict(X_val)\n",
    "\n",
    "# Ensure predictions are positive (required for log calculation)\n",
    "y_pred = np.maximum(y_pred, 0)\n",
    "\n",
    "# Calculate metrics\n",
    "rmsle_score = rmsle(y_val, y_pred)\n",
    "rmse_score = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "print(f\"Basic XGBoost Model Performance:\")\n",
    "print(f\"RMSLE: {rmsle_score:.4f}\")\n",
    "print(f\"RMSE: {rmse_score:.4f}\")\n",
    "print(f\"RÂ² Score: {r2:.4f}\")\n"
   ],
   "id": "235417a0cd724770"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': basic_xgb.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "fd8899445fda484d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Hyperparameter Tuning with RandomizedSearchCV\n",
   "id": "94e156447681a357"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3],\n",
    "    'min_child_weight': [1, 3, 5, 7],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1],\n",
    "    'reg_lambda': [0, 0.1, 0.5, 1]\n",
    "}\n",
    "\n",
    "# Create a custom scorer for RMSLE\n",
    "from sklearn.metrics import make_scorer\n",
    "rmsle_scorer = make_scorer(rmsle, greater_is_better=False)\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "xgb_random = RandomizedSearchCV(\n",
    "    estimator=xgb.XGBRegressor(objective='reg:squarederror', random_state=42),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,  # Number of parameter settings sampled\n",
    "    scoring=rmsle_scorer,\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the randomized search\n",
    "xgb_random.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\")\n",
    "print(xgb_random.best_params_)\n",
    "print(f\"Best RMSLE: {-xgb_random.best_score_:.4f}\")\n"
   ],
   "id": "65109f4e155b9db8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. Train the Optimized XGBoost Model\n",
   "id": "44ca7f1211275154"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train the model with the best parameters\n",
    "best_xgb_model = xgb.XGBRegressor(**xgb_random.best_params_, random_state=42)\n",
    "best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_best = best_xgb_model.predict(X_val)\n",
    "\n",
    "# Ensure predictions are positive\n",
    "y_pred_best = np.maximum(y_pred_best, 0)\n",
    "\n",
    "# Calculate metrics\n",
    "rmsle_score_best = rmsle(y_val, y_pred_best)\n",
    "rmse_score_best = np.sqrt(mean_squared_error(y_val, y_pred_best))\n",
    "r2_best = r2_score(y_val, y_pred_best)\n",
    "\n",
    "print(f\"Optimized XGBoost Model Performance:\")\n",
    "print(f\"RMSLE: {rmsle_score_best:.4f}\")\n",
    "print(f\"RMSE: {rmse_score_best:.4f}\")\n",
    "print(f\"RÂ² Score: {r2_best:.4f}\")\n",
    "\n",
    "# Compare with the basic model\n",
    "print(\"\\nImprovement over basic model:\")\n",
    "print(f\"RMSLE improvement: {rmsle_score - rmsle_score_best:.4f} ({(rmsle_score - rmsle_score_best) / rmsle_score * 100:.2f}%)\")\n"
   ],
   "id": "c95e821514421d59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Feature importance of the optimized model\n",
    "feature_importance_best = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': best_xgb_model.feature_importances_\n",
    "})\n",
    "feature_importance_best = feature_importance_best.sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_best)\n",
    "plt.title('Feature Importance (Optimized Model)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "829ad366cc0c99c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9. Visualize Predictions vs Actual Values\n",
   "id": "eb80dae22757ffa5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a dataframe with actual and predicted values\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': y_val,\n",
    "    'Predicted': y_pred_best\n",
    "})\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(results_df['Actual'], results_df['Predicted'], alpha=0.5)\n",
    "plt.plot([min(y_val), max(y_val)], [min(y_val), max(y_val)], 'r--')\n",
    "plt.xlabel('Actual Calories')\n",
    "plt.ylabel('Predicted Calories')\n",
    "plt.title('Actual vs Predicted Calories')\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals\n",
    "results_df['Residuals'] = results_df['Actual'] - results_df['Predicted']\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(results_df['Residuals'], kde=True)\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.title('Residuals Distribution')\n",
    "plt.xlabel('Residuals')\n",
    "plt.show()\n"
   ],
   "id": "61349658773f5f8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 10. Make Predictions on Test Data\n",
   "id": "d65b7610b8f59ec3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prepare test data with the same features\n",
    "X_test = test_fe[features]\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Ensure predictions are positive\n",
    "test_predictions = np.maximum(test_predictions, 0)\n",
    "\n",
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'Calories': test_predictions\n",
    "})\n",
    "\n",
    "# Display the first few rows of the submission file\n",
    "submission.head()\n"
   ],
   "id": "bcf3700f9e80b053"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save the submission file\n",
    "submission.to_csv('xgboost_submission.csv', index=False)\n",
    "print(\"Submission file saved successfully!\")\n"
   ],
   "id": "8003b2784c689088"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "In this notebook, we built an XGBoost model to predict calories burned during workouts. We performed the following steps:\n",
    "\n",
    "1. Loaded and explored the dataset\n",
    "2. Conducted exploratory data analysis to understand the relationships between features and the target variable\n",
    "3. Performed feature engineering to create new features that might improve model performance\n",
    "4. Trained a basic XGBoost model as a baseline\n",
    "5. Used RandomizedSearchCV to tune the hyperparameters of the model\n",
    "6. Trained an optimized XGBoost model with the best hyperparameters\n",
    "7. Evaluated the model's performance using RMSLE (Root Mean Squared Logarithmic Error)\n",
    "8. Generated predictions for the test set and created a submission file\n",
    "\n",
    "The optimized XGBoost model showed good performance on the validation set, with a significant improvement over the baseline model. The most important features for predicting calories burned were identified through the model's built-in feature importance."
   ],
   "id": "9b77379344b60cfb"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
