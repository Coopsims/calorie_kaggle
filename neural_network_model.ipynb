{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Calories Prediction - Neural Network Model\n",
    "\n",
    "This notebook implements a Neural Network model to predict calories burned during workouts for the Kaggle Playground Series competition.\n"
   ],
   "id": "9bdc29dcf3f628c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Import Libraries\n",
   "id": "b0cea4e08deccc36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T00:36:23.872885Z",
     "start_time": "2025-05-09T00:36:23.829462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error, r2_score\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import multiprocessing\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42) if torch.cuda.is_available() else None\n",
    "\n",
    "# Set device - check for CUDA first, then MPS (for Mac M1/M2), then fall back to CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Metal Performance Shaders) device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device\")\n",
    "\n",
    "# Set up multiprocessing for CPU if needed\n",
    "num_workers = 0\n",
    "if device.type == 'cpu':\n",
    "    # Set multiprocessing start method (for Jupyter notebook compatibility)\n",
    "    try:\n",
    "        multiprocessing.set_start_method('spawn', force=True)\n",
    "        print(\"Using 'spawn' start method for multiprocessing\")\n",
    "    except RuntimeError:\n",
    "        print(\"Could not set start method to 'spawn' for multiprocessing\")\n",
    "\n",
    "    # Set number of workers for DataLoader\n",
    "    num_workers = multiprocessing.cpu_count() - 1  # Leave one CPU for system tasks\n",
    "    num_workers = max(0, num_workers)  # Ensure non-negative\n",
    "\n",
    "    # Enable multiprocessing for PyTorch\n",
    "    torch.set_num_threads(num_workers)\n",
    "    print(f\"Using {num_workers} workers for data loading and {torch.get_num_threads()} threads for PyTorch operations\")\n"
   ],
   "id": "d5763931fa549066",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Metal Performance Shaders) device\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Load and Explore Data\n",
   "id": "1fa3599c6fa61395"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T00:36:26.999122Z",
     "start_time": "2025-05-09T00:36:26.748747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the training and test data\n",
    "train_data = pd.read_csv('playground-series-s5e5/train.csv')\n",
    "test_data = pd.read_csv('playground-series-s5e5/test.csv')\n",
    "\n",
    "# Display basic information about the training data\n",
    "print(\"Training data shape:\", train_data.shape)\n",
    "train_data.head()\n"
   ],
   "id": "3ce3efb20292beb2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (750000, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   id     Sex  Age  Height  Weight  Duration  Heart_Rate  Body_Temp  Calories\n",
       "0   0    male   36   189.0    82.0      26.0       101.0       41.0     150.0\n",
       "1   1  female   64   163.0    60.0       8.0        85.0       39.7      34.0\n",
       "2   2  female   51   161.0    64.0       7.0        84.0       39.8      29.0\n",
       "3   3    male   20   192.0    90.0      25.0       105.0       40.7     140.0\n",
       "4   4  female   38   166.0    61.0      25.0       102.0       40.6     146.0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Heart_Rate</th>\n",
       "      <th>Body_Temp</th>\n",
       "      <th>Calories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>36</td>\n",
       "      <td>189.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>64</td>\n",
       "      <td>163.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>39.7</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>51</td>\n",
       "      <td>161.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>20</td>\n",
       "      <td>192.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>40.7</td>\n",
       "      <td>140.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>166.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>40.6</td>\n",
       "      <td>146.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T00:36:28.369553Z",
     "start_time": "2025-05-09T00:36:28.339214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in training data:\")\n",
    "print(train_data.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in test data:\")\n",
    "print(test_data.isnull().sum())\n"
   ],
   "id": "b6f70d6495c82334",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in training data:\n",
      "id            0\n",
      "Sex           0\n",
      "Age           0\n",
      "Height        0\n",
      "Weight        0\n",
      "Duration      0\n",
      "Heart_Rate    0\n",
      "Body_Temp     0\n",
      "Calories      0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in test data:\n",
      "id            0\n",
      "Sex           0\n",
      "Age           0\n",
      "Height        0\n",
      "Weight        0\n",
      "Duration      0\n",
      "Heart_Rate    0\n",
      "Body_Temp     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T00:36:29.907439Z",
     "start_time": "2025-05-09T00:36:29.795117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Statistical summary of the training data\n",
    "train_data.describe()\n"
   ],
   "id": "a0aec71a4010d24",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  id            Age         Height         Weight  \\\n",
       "count  750000.000000  750000.000000  750000.000000  750000.000000   \n",
       "mean   374999.500000      41.420404     174.697685      75.145668   \n",
       "std    216506.495284      15.175049      12.824496      13.982704   \n",
       "min         0.000000      20.000000     126.000000      36.000000   \n",
       "25%    187499.750000      28.000000     164.000000      63.000000   \n",
       "50%    374999.500000      40.000000     174.000000      74.000000   \n",
       "75%    562499.250000      52.000000     185.000000      87.000000   \n",
       "max    749999.000000      79.000000     222.000000     132.000000   \n",
       "\n",
       "            Duration     Heart_Rate      Body_Temp       Calories  \n",
       "count  750000.000000  750000.000000  750000.000000  750000.000000  \n",
       "mean       15.421015      95.483995      40.036253      88.282781  \n",
       "std         8.354095       9.449845       0.779875      62.395349  \n",
       "min         1.000000      67.000000      37.100000       1.000000  \n",
       "25%         8.000000      88.000000      39.600000      34.000000  \n",
       "50%        15.000000      95.000000      40.300000      77.000000  \n",
       "75%        23.000000     103.000000      40.700000     136.000000  \n",
       "max        30.000000     128.000000      41.500000     314.000000  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Heart_Rate</th>\n",
       "      <th>Body_Temp</th>\n",
       "      <th>Calories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>750000.000000</td>\n",
       "      <td>750000.000000</td>\n",
       "      <td>750000.000000</td>\n",
       "      <td>750000.000000</td>\n",
       "      <td>750000.000000</td>\n",
       "      <td>750000.000000</td>\n",
       "      <td>750000.000000</td>\n",
       "      <td>750000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>374999.500000</td>\n",
       "      <td>41.420404</td>\n",
       "      <td>174.697685</td>\n",
       "      <td>75.145668</td>\n",
       "      <td>15.421015</td>\n",
       "      <td>95.483995</td>\n",
       "      <td>40.036253</td>\n",
       "      <td>88.282781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>216506.495284</td>\n",
       "      <td>15.175049</td>\n",
       "      <td>12.824496</td>\n",
       "      <td>13.982704</td>\n",
       "      <td>8.354095</td>\n",
       "      <td>9.449845</td>\n",
       "      <td>0.779875</td>\n",
       "      <td>62.395349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>37.100000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>187499.750000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>39.600000</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>374999.500000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>40.300000</td>\n",
       "      <td>77.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>562499.250000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>40.700000</td>\n",
       "      <td>136.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>749999.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>41.500000</td>\n",
       "      <td>314.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Feature Engineering\n",
   "id": "baa8f38684bfb9ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T00:36:31.528266Z",
     "start_time": "2025-05-09T00:36:31.474588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a copy of the datasets for feature engineering\n",
    "train_fe = train_data.copy()\n",
    "test_fe = test_data.copy()\n",
    "\n",
    "# Calculate BMI (Body Mass Index)\n",
    "train_fe['BMI'] = train_fe['Weight'] / ((train_fe['Height'] / 100) ** 2)\n",
    "test_fe['BMI'] = test_fe['Weight'] / ((test_fe['Height'] / 100) ** 2)\n",
    "\n",
    "# Create interaction features\n",
    "train_fe['Duration_HeartRate'] = train_fe['Duration'] * train_fe['Heart_Rate']\n",
    "test_fe['Duration_HeartRate'] = test_fe['Duration'] * test_fe['Heart_Rate']\n",
    "\n",
    "train_fe['Weight_Duration'] = train_fe['Weight'] * train_fe['Duration']\n",
    "test_fe['Weight_Duration'] = test_fe['Weight'] * test_fe['Duration']\n",
    "\n",
    "# Convert Sex to numerical (0 for female, 1 for male)\n",
    "train_fe['Sex_num'] = train_fe['Sex'].map({'female': 0, 'male': 1})\n",
    "test_fe['Sex_num'] = test_fe['Sex'].map({'female': 0, 'male': 1})\n",
    "\n",
    "# Display the new features\n",
    "train_fe.head()\n"
   ],
   "id": "538d8f406260cc25",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   id     Sex  Age  Height  Weight  Duration  Heart_Rate  Body_Temp  Calories  \\\n",
       "0   0    male   36   189.0    82.0      26.0       101.0       41.0     150.0   \n",
       "1   1  female   64   163.0    60.0       8.0        85.0       39.7      34.0   \n",
       "2   2  female   51   161.0    64.0       7.0        84.0       39.8      29.0   \n",
       "3   3    male   20   192.0    90.0      25.0       105.0       40.7     140.0   \n",
       "4   4  female   38   166.0    61.0      25.0       102.0       40.6     146.0   \n",
       "\n",
       "         BMI  Duration_HeartRate  Weight_Duration  Sex_num  \n",
       "0  22.955684              2626.0           2132.0        1  \n",
       "1  22.582709               680.0            480.0        0  \n",
       "2  24.690405               588.0            448.0        0  \n",
       "3  24.414062              2625.0           2250.0        1  \n",
       "4  22.136740              2550.0           1525.0        0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Heart_Rate</th>\n",
       "      <th>Body_Temp</th>\n",
       "      <th>Calories</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Duration_HeartRate</th>\n",
       "      <th>Weight_Duration</th>\n",
       "      <th>Sex_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>36</td>\n",
       "      <td>189.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>22.955684</td>\n",
       "      <td>2626.0</td>\n",
       "      <td>2132.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>64</td>\n",
       "      <td>163.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>39.7</td>\n",
       "      <td>34.0</td>\n",
       "      <td>22.582709</td>\n",
       "      <td>680.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>51</td>\n",
       "      <td>161.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>29.0</td>\n",
       "      <td>24.690405</td>\n",
       "      <td>588.0</td>\n",
       "      <td>448.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>20</td>\n",
       "      <td>192.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>40.7</td>\n",
       "      <td>140.0</td>\n",
       "      <td>24.414062</td>\n",
       "      <td>2625.0</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>166.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>40.6</td>\n",
       "      <td>146.0</td>\n",
       "      <td>22.136740</td>\n",
       "      <td>2550.0</td>\n",
       "      <td>1525.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Prepare Data for Modeling\n",
   "id": "e7a403866f863b5b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T00:36:33.931385Z",
     "start_time": "2025-05-09T00:36:33.867139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define features and target\n",
    "features = ['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp', 'Sex_num', 'BMI', 'Duration_HeartRate', 'Weight_Duration']\n",
    "X = train_fe[features]\n",
    "y = train_fe['Calories']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n"
   ],
   "id": "83da1a7b505cb9e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (600000, 10)\n",
      "Validation set shape: (150000, 10)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T00:36:35.724924Z",
     "start_time": "2025-05-09T00:36:35.666854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Prepare test data\n",
    "X_test = test_fe[features]\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ],
   "id": "a275dcecbb24797c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Build a Basic Neural Network Model\n",
   "id": "774cf6a039d5ed51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T00:36:37.540908Z",
     "start_time": "2025-05-09T00:36:37.535387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a PyTorch neural network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers=[64, 32], activation='relu', \n",
    "                 dropout_rate=0.2, l1_reg=0.0, l2_reg=0.0):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # Set activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'elu':\n",
    "            self.activation = nn.ELU()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        else:\n",
    "            self.activation = nn.ReLU()  # Default to ReLU\n",
    "\n",
    "        # Create layers list\n",
    "        layers = []\n",
    "\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_dim, hidden_layers[0]))\n",
    "        layers.append(self.activation)\n",
    "        layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_layers) - 1):\n",
    "            layers.append(nn.Linear(hidden_layers[i], hidden_layers[i+1]))\n",
    "            layers.append(self.activation)\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_layers[-1], 1))\n",
    "\n",
    "        # Create sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "        # Store regularization parameters\n",
    "        self.l1_reg = l1_reg\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def l1_regularization(self):\n",
    "        l1_loss = 0\n",
    "        for param in self.parameters():\n",
    "            l1_loss += torch.sum(torch.abs(param))\n",
    "        return self.l1_reg * l1_loss\n",
    "\n",
    "    def l2_regularization(self):\n",
    "        l2_loss = 0\n",
    "        for param in self.parameters():\n",
    "            l2_loss += torch.sum(param.pow(2))\n",
    "        return self.l2_reg * l2_loss\n",
    "\n",
    "# Function to create and configure the model\n",
    "def create_model(input_dim, hidden_layers=[64, 32], activation='relu', \n",
    "                 learning_rate=0.001, dropout_rate=0.2, l1_reg=0.0, l2_reg=0.0):\n",
    "    # Create model\n",
    "    model = NeuralNetwork(\n",
    "        input_dim=input_dim,\n",
    "        hidden_layers=hidden_layers,\n",
    "        activation=activation,\n",
    "        dropout_rate=dropout_rate,\n",
    "        l1_reg=l1_reg,\n",
    "        l2_reg=l2_reg\n",
    "    ).to(device)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    return model, optimizer\n"
   ],
   "id": "6a29ec58d570aeb0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T00:36:42.735215Z",
     "start_time": "2025-05-09T00:36:40.888834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).to(device).reshape(-1, 1)\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val.values).to(device).reshape(-1, 1)\n",
    "\n",
    "# Create a basic neural network model\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "basic_model, basic_optimizer = create_model(input_dim)\n",
    "\n",
    "# Display model summary\n",
    "print(basic_model)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in basic_model.parameters())}\")\n"
   ],
   "id": "fa82bd0a07a45379",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (activation): ReLU()\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Total parameters: 2817\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-09T00:40:20.181197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a function for mean squared logarithmic error\n",
    "def msle_loss(pred, target):\n",
    "    # Ensure predictions are positive\n",
    "    pred = torch.clamp(pred, min=1e-6)\n",
    "    target = torch.clamp(target, min=1e-6)\n",
    "\n",
    "    # Calculate MSLE\n",
    "    return torch.mean(torch.pow(torch.log1p(pred) - torch.log1p(target), 2))\n",
    "\n",
    "# Define a function for training with early stopping\n",
    "def train_model(model, optimizer, X_train, y_train, X_val, y_val, \n",
    "                epochs=100, batch_size=32, patience=20, factor=0.2, min_lr=0.0001, \n",
    "                verbose=1):\n",
    "    \"\"\"\n",
    "    Train a PyTorch model with early stopping and learning rate reduction.\n",
    "\n",
    "    Parameters:\n",
    "    - model: PyTorch model to train\n",
    "    - optimizer: PyTorch optimizer\n",
    "    - X_train, y_train: Training data\n",
    "    - X_val, y_val: Validation data\n",
    "    - epochs: Maximum number of epochs to train\n",
    "    - batch_size: Batch size for training\n",
    "    - patience: Number of epochs with no improvement after which training will be stopped\n",
    "    - factor: Factor by which the learning rate will be reduced\n",
    "    - min_lr: Minimum learning rate\n",
    "    - verbose: Verbosity level (0=silent, 1=normal, 2=detailed)\n",
    "\n",
    "    Returns:\n",
    "    - model: Trained model\n",
    "    - history: Training history\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from datetime import timedelta\n",
    "    import numpy as np\n",
    "\n",
    "    # Initialize variables for early stopping and learning rate reduction\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_mses = []\n",
    "    val_mses = []\n",
    "\n",
    "    # Time tracking\n",
    "    start_time = time.time()\n",
    "    epoch_times = []\n",
    "\n",
    "    # Print training information\n",
    "    if verbose > 0:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Starting training with {epochs} epochs, batch size {batch_size}\")\n",
    "        print(f\"Device: {device}, Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "        print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n",
    "        print(f\"Learning rate: {optimizer.param_groups[0]['lr']}, Patience: {patience}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=num_workers if device.type == 'cpu' else 0,\n",
    "        pin_memory=(device.type != 'cpu')\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_mse = 0\n",
    "\n",
    "        # Batch progress tracking\n",
    "        if verbose > 1:\n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs}:\")\n",
    "            print(f\"{'='*50}\")\n",
    "\n",
    "        # Track batch progress\n",
    "        total_batches = len(train_loader)\n",
    "\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            # Forward pass\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = msle_loss(y_pred, y_batch)\n",
    "\n",
    "            # Add regularization if needed\n",
    "            if model.l1_reg > 0:\n",
    "                l1_loss = model.l1_regularization()\n",
    "                loss += l1_loss\n",
    "            if model.l2_reg > 0:\n",
    "                l2_loss = model.l2_regularization()\n",
    "                loss += l2_loss\n",
    "\n",
    "            # Calculate MSE for tracking\n",
    "            mse = torch.mean(torch.pow(y_pred - y_batch, 2))\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate batch loss\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "            train_mse += mse.item() * X_batch.size(0)\n",
    "\n",
    "            # Print batch progress\n",
    "            if verbose > 1 and (batch_idx % max(1, total_batches // 10) == 0 or batch_idx == total_batches - 1):\n",
    "                batch_loss = loss.item()\n",
    "                batch_mse = mse.item()\n",
    "                progress = (batch_idx + 1) / total_batches * 100\n",
    "                print(f\"Batch {batch_idx+1}/{total_batches} [{progress:.1f}%] - Loss: {batch_loss:.6f}, MSE: {batch_mse:.6f}\")\n",
    "\n",
    "        # Calculate average training loss\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_mse /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        train_mses.append(train_mse)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = model(X_val)\n",
    "            val_loss = msle_loss(y_val_pred, y_val).item()\n",
    "            val_mse = torch.mean(torch.pow(y_val_pred - y_val, 2)).item()\n",
    "\n",
    "            # Calculate RMSLE for more detailed reporting\n",
    "            y_val_np = y_val.cpu().numpy()\n",
    "            y_val_pred_np = y_val_pred.cpu().numpy().flatten()\n",
    "            y_val_pred_np = np.maximum(y_val_pred_np, 0)  # Ensure predictions are positive\n",
    "            val_rmsle = np.sqrt(mean_squared_log_error(y_val_np, y_val_pred_np))\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        val_mses.append(val_mse)\n",
    "\n",
    "        # Calculate epoch time\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_time = epoch_end_time - epoch_start_time\n",
    "        epoch_times.append(epoch_time)\n",
    "\n",
    "        # Calculate estimated time remaining\n",
    "        avg_epoch_time = np.mean(epoch_times)\n",
    "        remaining_epochs = epochs - (epoch + 1)\n",
    "        estimated_time_remaining = avg_epoch_time * remaining_epochs\n",
    "\n",
    "        # Print progress\n",
    "        if verbose > 0 and (epoch % max(1, epochs // 20) == 0 or epoch == epochs - 1):\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - {timedelta(seconds=int(epoch_time))} - ETA: {timedelta(seconds=int(estimated_time_remaining))}\")\n",
    "            print(f\"  Train Loss: {train_loss:.6f}, Train MSE: {train_mse:.6f}\")\n",
    "            print(f\"  Val Loss: {val_loss:.6f}, Val MSE: {val_mse:.6f}, Val RMSLE: {val_rmsle:.6f}\")\n",
    "\n",
    "            # Print current learning rate\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"  Learning rate: {current_lr:.6f}\")\n",
    "\n",
    "            # Print improvement information\n",
    "            if epoch > 0:\n",
    "                prev_val_loss = val_losses[-2]\n",
    "                loss_change = (prev_val_loss - val_loss) / prev_val_loss * 100\n",
    "                change_sign = \"↓\" if loss_change > 0 else \"↑\"\n",
    "                print(f\"  Validation loss change: {change_sign} {abs(loss_change):.2f}%\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            improvement = (best_val_loss - val_loss) / best_val_loss * 100\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(f\"  ✓ New best validation loss: {best_val_loss:.6f} (improved by {improvement:.2f}%)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if verbose > 0:\n",
    "                print(f\"  ✗ No improvement for {patience_counter}/{patience} epochs. Best: {best_val_loss:.6f}\")\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\n{'='*50}\")\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "                print(f\"{'='*50}\")\n",
    "                break\n",
    "\n",
    "        # Learning rate reduction\n",
    "        if patience_counter > 0 and patience_counter % 5 == 0:\n",
    "            old_lr = optimizer.param_groups[0]['lr']\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = max(param_group['lr'] * factor, min_lr)\n",
    "            new_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(f\"  → Reducing learning rate: {old_lr:.6f} → {new_lr:.6f}\")\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Calculate total training time\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training completed in {timedelta(seconds=int(total_time))}\")\n",
    "        print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "        print(f\"Total epochs: {len(train_losses)}/{epochs}\")\n",
    "        print(f\"Average epoch time: {timedelta(seconds=int(np.mean(epoch_times)))}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # Return training history\n",
    "    history = {\n",
    "        'loss': train_losses,\n",
    "        'val_loss': val_losses,\n",
    "        'mse': train_mses,\n",
    "        'val_mse': val_mses,\n",
    "        'epoch_times': epoch_times,\n",
    "        'total_time': total_time,\n",
    "        'best_val_loss': best_val_loss\n",
    "    }\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# Train the basic model\n",
    "basic_model, history = train_model(\n",
    "    basic_model, basic_optimizer,\n",
    "    X_train_tensor, y_train_tensor,\n",
    "    X_val_tensor, y_val_tensor,\n",
    "    epochs=100,\n",
    "    batch_size=1000,\n",
    "    verbose=2  # Set to 2 for more detailed output\n",
    ")\n"
   ],
   "id": "9ec2b7d1eb5adce5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Starting training with 100 epochs, batch size 1000\n",
      "Device: mps, Model parameters: 2817\n",
      "Training samples: 600000, Validation samples: 150000\n",
      "Learning rate: 0.001, Patience: 20\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Epoch 1/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.019510, MSE: 230.345383\n",
      "Batch 61/600 [10.2%] - Loss: 0.021878, MSE: 263.829834\n",
      "Batch 121/600 [20.2%] - Loss: 0.018556, MSE: 237.502106\n",
      "Batch 181/600 [30.2%] - Loss: 0.021451, MSE: 244.836761\n",
      "Batch 241/600 [40.2%] - Loss: 0.022569, MSE: 232.421295\n",
      "Batch 301/600 [50.2%] - Loss: 0.030044, MSE: 216.616135\n",
      "Batch 361/600 [60.2%] - Loss: 0.022076, MSE: 273.329132\n",
      "Batch 421/600 [70.2%] - Loss: 0.020186, MSE: 226.174316\n",
      "Batch 481/600 [80.2%] - Loss: 0.019562, MSE: 254.799820\n",
      "Batch 541/600 [90.2%] - Loss: 0.019227, MSE: 227.667755\n",
      "Batch 600/600 [100.0%] - Loss: 0.017921, MSE: 218.830795\n",
      "Epoch 1/100 - 0:00:12 - ETA: 0:19:50\n",
      "  Train Loss: 0.020995, Train MSE: 234.900398\n",
      "  Val Loss: 0.004555, Val MSE: 17.878750, Val RMSLE: 0.067491\n",
      "  Learning rate: 0.001000\n",
      "  ✓ New best validation loss: 0.004555 (improved by nan%)\n",
      "\n",
      "Epoch 2/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.021331, MSE: 240.144196\n",
      "Batch 61/600 [10.2%] - Loss: 0.020047, MSE: 204.834564\n",
      "Batch 121/600 [20.2%] - Loss: 0.021576, MSE: 260.769257\n",
      "Batch 181/600 [30.2%] - Loss: 0.018767, MSE: 215.083633\n",
      "Batch 241/600 [40.2%] - Loss: 0.023418, MSE: 254.728531\n",
      "Batch 301/600 [50.2%] - Loss: 0.021397, MSE: 252.649231\n",
      "Batch 361/600 [60.2%] - Loss: 0.023703, MSE: 246.771103\n",
      "Batch 421/600 [70.2%] - Loss: 0.020899, MSE: 250.654266\n",
      "Batch 481/600 [80.2%] - Loss: 0.019188, MSE: 214.632385\n",
      "Batch 541/600 [90.2%] - Loss: 0.021774, MSE: 252.500824\n",
      "Batch 600/600 [100.0%] - Loss: 0.021312, MSE: 251.012924\n",
      "  ✗ No improvement for 1/20 epochs. Best: 0.004555\n",
      "\n",
      "Epoch 3/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.020614, MSE: 236.609360\n",
      "Batch 61/600 [10.2%] - Loss: 0.021260, MSE: 253.588165\n",
      "Batch 121/600 [20.2%] - Loss: 0.018569, MSE: 213.576691\n",
      "Batch 181/600 [30.2%] - Loss: 0.021191, MSE: 246.860153\n",
      "Batch 241/600 [40.2%] - Loss: 0.019299, MSE: 200.739899\n",
      "Batch 301/600 [50.2%] - Loss: 0.019781, MSE: 227.539017\n",
      "Batch 361/600 [60.2%] - Loss: 0.021746, MSE: 257.398621\n",
      "Batch 421/600 [70.2%] - Loss: 0.018942, MSE: 206.320389\n",
      "Batch 481/600 [80.2%] - Loss: 0.021411, MSE: 221.062881\n",
      "Batch 541/600 [90.2%] - Loss: 0.020982, MSE: 242.102982\n",
      "Batch 600/600 [100.0%] - Loss: 0.018422, MSE: 243.679993\n",
      "  ✗ No improvement for 2/20 epochs. Best: 0.004555\n",
      "\n",
      "Epoch 4/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.024803, MSE: 234.497330\n",
      "Batch 61/600 [10.2%] - Loss: 0.022272, MSE: 237.328110\n",
      "Batch 121/600 [20.2%] - Loss: 0.021999, MSE: 234.212601\n",
      "Batch 181/600 [30.2%] - Loss: 0.019310, MSE: 228.049225\n",
      "Batch 241/600 [40.2%] - Loss: 0.022025, MSE: 251.063538\n",
      "Batch 301/600 [50.2%] - Loss: 0.020636, MSE: 238.485214\n",
      "Batch 361/600 [60.2%] - Loss: 0.020030, MSE: 242.042297\n",
      "Batch 421/600 [70.2%] - Loss: 0.021899, MSE: 217.279572\n",
      "Batch 481/600 [80.2%] - Loss: 0.019525, MSE: 206.763000\n",
      "Batch 541/600 [90.2%] - Loss: 0.017672, MSE: 213.850006\n",
      "Batch 600/600 [100.0%] - Loss: 0.019696, MSE: 239.868515\n",
      "  ✗ No improvement for 3/20 epochs. Best: 0.004555\n",
      "\n",
      "Epoch 5/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.019702, MSE: 198.941574\n",
      "Batch 61/600 [10.2%] - Loss: 0.019708, MSE: 219.439316\n",
      "Batch 121/600 [20.2%] - Loss: 0.018657, MSE: 217.031281\n",
      "Batch 181/600 [30.2%] - Loss: 0.021305, MSE: 252.524094\n",
      "Batch 241/600 [40.2%] - Loss: 0.020931, MSE: 205.109070\n",
      "Batch 301/600 [50.2%] - Loss: 0.021211, MSE: 214.119827\n",
      "Batch 361/600 [60.2%] - Loss: 0.020810, MSE: 228.625763\n",
      "Batch 421/600 [70.2%] - Loss: 0.020148, MSE: 229.555527\n",
      "Batch 481/600 [80.2%] - Loss: 0.023307, MSE: 243.599258\n",
      "Batch 541/600 [90.2%] - Loss: 0.019184, MSE: 210.811508\n",
      "Batch 600/600 [100.0%] - Loss: 0.019975, MSE: 228.679016\n",
      "  ✓ New best validation loss: 0.004555 (improved by 0.01%)\n",
      "\n",
      "Epoch 6/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.019912, MSE: 201.219757\n",
      "Batch 61/600 [10.2%] - Loss: 0.021296, MSE: 244.077545\n",
      "Batch 121/600 [20.2%] - Loss: 0.018223, MSE: 205.330017\n",
      "Batch 181/600 [30.2%] - Loss: 0.023016, MSE: 252.183075\n",
      "Batch 241/600 [40.2%] - Loss: 0.020306, MSE: 205.507324\n",
      "Batch 301/600 [50.2%] - Loss: 0.019093, MSE: 229.250778\n",
      "Batch 361/600 [60.2%] - Loss: 0.019635, MSE: 227.100281\n",
      "Batch 421/600 [70.2%] - Loss: 0.021151, MSE: 234.948624\n",
      "Batch 481/600 [80.2%] - Loss: 0.017201, MSE: 206.045609\n",
      "Batch 541/600 [90.2%] - Loss: 0.019250, MSE: 219.872910\n",
      "Batch 600/600 [100.0%] - Loss: 0.019224, MSE: 219.127731\n",
      "Epoch 6/100 - 0:00:11 - ETA: 0:18:21\n",
      "  Train Loss: 0.020464, Train MSE: 229.542655\n",
      "  Val Loss: 0.004643, Val MSE: 18.242666, Val RMSLE: 0.068142\n",
      "  Learning rate: 0.001000\n",
      "  Validation loss change: ↑ 1.95%\n",
      "  ✗ No improvement for 1/20 epochs. Best: 0.004555\n",
      "\n",
      "Epoch 7/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.020039, MSE: 239.778717\n",
      "Batch 61/600 [10.2%] - Loss: 0.020168, MSE: 221.808472\n",
      "Batch 121/600 [20.2%] - Loss: 0.022528, MSE: 216.276382\n",
      "Batch 181/600 [30.2%] - Loss: 0.019845, MSE: 231.323914\n",
      "Batch 241/600 [40.2%] - Loss: 0.020288, MSE: 201.879791\n",
      "Batch 301/600 [50.2%] - Loss: 0.021481, MSE: 243.277817\n",
      "Batch 361/600 [60.2%] - Loss: 0.019122, MSE: 264.670074\n",
      "Batch 421/600 [70.2%] - Loss: 0.019554, MSE: 225.329758\n",
      "Batch 481/600 [80.2%] - Loss: 0.018006, MSE: 196.050735\n",
      "Batch 541/600 [90.2%] - Loss: 0.021360, MSE: 244.130295\n",
      "Batch 600/600 [100.0%] - Loss: 0.020790, MSE: 249.761368\n",
      "  ✗ No improvement for 2/20 epochs. Best: 0.004555\n",
      "\n",
      "Epoch 8/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.020045, MSE: 201.518570\n",
      "Batch 61/600 [10.2%] - Loss: 0.022113, MSE: 227.243347\n",
      "Batch 121/600 [20.2%] - Loss: 0.020402, MSE: 210.528580\n",
      "Batch 181/600 [30.2%] - Loss: 0.019398, MSE: 243.382889\n",
      "Batch 241/600 [40.2%] - Loss: 0.026892, MSE: 293.201935\n",
      "Batch 301/600 [50.2%] - Loss: 0.021928, MSE: 218.916016\n",
      "Batch 361/600 [60.2%] - Loss: 0.018400, MSE: 235.597122\n",
      "Batch 421/600 [70.2%] - Loss: 0.021307, MSE: 255.847290\n",
      "Batch 481/600 [80.2%] - Loss: 0.017877, MSE: 207.783447\n",
      "Batch 541/600 [90.2%] - Loss: 0.018596, MSE: 200.218979\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['loss'], label='Training Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['mse'], label='Training MSE')\n",
    "plt.plot(history['val_mse'], label='Validation MSE')\n",
    "plt.title('Mean Squared Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "ad21007a857d5100"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Make predictions on the validation set\n",
    "basic_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_basic = basic_model(X_val_tensor).cpu().numpy().flatten()\n",
    "\n",
    "# Ensure predictions are positive (required for log calculation)\n",
    "y_pred_basic = np.maximum(y_pred_basic, 0)\n",
    "\n",
    "# Calculate RMSLE (Root Mean Squared Logarithmic Error)\n",
    "def rmsle(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "# Calculate metrics\n",
    "rmsle_score = rmsle(y_val, y_pred_basic)\n",
    "rmse_score = np.sqrt(mean_squared_error(y_val, y_pred_basic))\n",
    "r2 = r2_score(y_val, y_pred_basic)\n",
    "\n",
    "print(f\"Basic Neural Network Model Performance:\")\n",
    "print(f\"RMSLE: {rmsle_score:.4f}\")\n",
    "print(f\"RMSE: {rmse_score:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n"
   ],
   "id": "214fb93cf19ebf61"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Hyperparameter Tuning\n",
   "id": "c57d55d540afce88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define a function to evaluate a model with given hyperparameters\n",
    "def evaluate_model(hidden_layers, activation, learning_rate, dropout_rate, l1_reg, l2_reg, batch_size):\n",
    "    # Create the model and optimizer\n",
    "    model, optimizer = create_model(\n",
    "        input_dim=input_dim,\n",
    "        hidden_layers=hidden_layers,\n",
    "        activation=activation,\n",
    "        learning_rate=learning_rate,\n",
    "        dropout_rate=dropout_rate,\n",
    "        l1_reg=l1_reg,\n",
    "        l2_reg=l2_reg\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model, history = train_model(\n",
    "        model, optimizer,\n",
    "        X_train_tensor, y_train_tensor,\n",
    "        X_val_tensor, y_val_tensor,\n",
    "        epochs=100,\n",
    "        batch_size=batch_size,\n",
    "        patience=20,\n",
    "        factor=0.2,\n",
    "        min_lr=0.0001,\n",
    "        verbose=1  # Set to 2 for more detailed output\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_val_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    y_pred = np.maximum(y_pred, 0)  # Ensure predictions are positive\n",
    "    rmsle_val = rmsle(y_val, y_pred)\n",
    "\n",
    "    return model, rmsle_val, history\n"
   ],
   "id": "bd71857105a70849"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define hyperparameter combinations to try\n",
    "hyperparameter_combinations = [\n",
    "    # Hidden layers, activation, learning rate, dropout rate, l1 reg, l2 reg, batch size\n",
    "    [[128, 64], 'relu', 0.001, 0.2, 0.0, 0.0, 32],\n",
    "    [[256, 128, 64], 'relu', 0.001, 0.3, 0.0, 0.001, 32],\n",
    "    [[128, 64, 32], 'elu', 0.0005, 0.2, 0.0, 0.001, 64],\n",
    "    [[64, 32], 'relu', 0.001, 0.1, 0.0001, 0.0, 32],\n",
    "    [[128, 64, 32, 16], 'relu', 0.001, 0.2, 0.0, 0.001, 32]\n",
    "]\n",
    "\n",
    "# Evaluate each combination\n",
    "results = []\n",
    "for i, params in enumerate(hyperparameter_combinations):\n",
    "    print(f\"Training model {i+1}/{len(hyperparameter_combinations)}...\")\n",
    "    model, rmsle_val, history = evaluate_model(*params)\n",
    "    results.append({\n",
    "        'model': model,\n",
    "        'params': params,\n",
    "        'rmsle': rmsle_val,\n",
    "        'history': history\n",
    "    })\n",
    "    print(f\"RMSLE: {rmsle_val:.4f}\")\n",
    "\n",
    "# Find the best model\n",
    "best_result = min(results, key=lambda x: x['rmsle'])\n",
    "print(\"\\nBest model parameters:\")\n",
    "print(f\"Hidden layers: {best_result['params'][0]}\")\n",
    "print(f\"Activation: {best_result['params'][1]}\")\n",
    "print(f\"Learning rate: {best_result['params'][2]}\")\n",
    "print(f\"Dropout rate: {best_result['params'][3]}\")\n",
    "print(f\"L1 regularization: {best_result['params'][4]}\")\n",
    "print(f\"L2 regularization: {best_result['params'][5]}\")\n",
    "print(f\"Batch size: {best_result['params'][6]}\")\n",
    "print(f\"RMSLE: {best_result['rmsle']:.4f}\")\n"
   ],
   "id": "1679956ecc77ae78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get the best model\n",
    "best_model = best_result['model']\n",
    "\n",
    "# Make predictions on the validation set\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_best = best_model(X_val_tensor).cpu().numpy().flatten()\n",
    "\n",
    "y_pred_best = np.maximum(y_pred_best, 0)  # Ensure predictions are positive\n",
    "\n",
    "# Calculate metrics\n",
    "rmsle_score_best = rmsle(y_val, y_pred_best)\n",
    "rmse_score_best = np.sqrt(mean_squared_error(y_val, y_pred_best))\n",
    "r2_best = r2_score(y_val, y_pred_best)\n",
    "\n",
    "print(f\"Best Neural Network Model Performance:\")\n",
    "print(f\"RMSLE: {rmsle_score_best:.4f}\")\n",
    "print(f\"RMSE: {rmse_score_best:.4f}\")\n",
    "print(f\"R² Score: {r2_best:.4f}\")\n",
    "\n",
    "# Compare with the basic model\n",
    "print(\"\\nImprovement over basic model:\")\n",
    "print(f\"RMSLE improvement: {rmsle_score - rmsle_score_best:.4f} ({(rmsle_score - rmsle_score_best) / rmsle_score * 100:.2f}%)\")\n"
   ],
   "id": "7f59ebbd8df83ac9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Visualize Predictions vs Actual Values\n",
   "id": "62773adef252ad65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a dataframe with actual and predicted values\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': y_val,\n",
    "    'Predicted': y_pred_best\n",
    "})\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(results_df['Actual'], results_df['Predicted'], alpha=0.5)\n",
    "plt.plot([min(y_val), max(y_val)], [min(y_val), max(y_val)], 'r--')\n",
    "plt.xlabel('Actual Calories')\n",
    "plt.ylabel('Predicted Calories')\n",
    "plt.title('Actual vs Predicted Calories')\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals\n",
    "results_df['Residuals'] = results_df['Actual'] - results_df['Predicted']\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(results_df['Residuals'], kde=True)\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.title('Residuals Distribution')\n",
    "plt.xlabel('Residuals')\n",
    "plt.show()\n"
   ],
   "id": "596b6d006a9b4ca1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. Make Predictions on Test Data\n",
   "id": "c84dbc89a639ef2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Convert test data to PyTorch tensor\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "\n",
    "# Make predictions on the test set\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = best_model(X_test_tensor).cpu().numpy().flatten()\n",
    "\n",
    "# Ensure predictions are positive\n",
    "test_predictions = np.maximum(test_predictions, 0)\n",
    "\n",
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'Calories': test_predictions\n",
    "})\n",
    "\n",
    "# Display the first few rows of the submission file\n",
    "submission.head()\n"
   ],
   "id": "9c563e611df2dd8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save the submission file\n",
    "submission.to_csv('neural_network_submission.csv', index=False)\n",
    "print(\"Submission file saved successfully!\")\n"
   ],
   "id": "d6ce570be7f85109"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9. Feature Importance Analysis (Permutation Importance)\n",
   "id": "ef81378a1dc8e371"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Since neural networks don't provide feature importance directly,\n",
    "# we can use permutation importance to estimate feature importance\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Create a wrapper for the PyTorch model to use with scikit-learn\n",
    "class PyTorchRegressor:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Convert numpy array to PyTorch tensor\n",
    "        X_tensor = torch.FloatTensor(X).to(self.device)\n",
    "\n",
    "        # Make predictions\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(X_tensor).cpu().numpy().flatten()\n",
    "\n",
    "        return predictions\n",
    "\n",
    "# Create a wrapper for our best model\n",
    "torch_wrapper = PyTorchRegressor(best_model, device)\n",
    "\n",
    "# Calculate permutation importance\n",
    "result = permutation_importance(\n",
    "    torch_wrapper, X_val_scaled, y_val,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Create a dataframe with feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': result.importances_mean\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "plt.title('Feature Importance (Permutation Importance)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "2acefe3755d57a0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "In this notebook, we built a Neural Network model to predict calories burned during workouts. We performed the following steps:\n",
    "\n",
    "1. Loaded and explored the dataset\n",
    "2. Performed feature engineering to create new features that might improve model performance\n",
    "3. Prepared the data for modeling, including scaling the features\n",
    "4. Built a basic neural network model as a baseline\n",
    "5. Tuned the hyperparameters of the model, including network architecture, activation functions, and regularization\n",
    "6. Trained an optimized neural network model with the best hyperparameters\n",
    "7. Evaluated the model's performance using RMSLE (Root Mean Squared Logarithmic Error)\n",
    "8. Analyzed feature importance using permutation importance\n",
    "9. Generated predictions for the test set and created a submission file\n",
    "\n",
    "The optimized neural network model showed good performance on the validation set, with an improvement over the baseline model. The most important features for predicting calories burned were identified through permutation importance analysis.\n"
   ],
   "id": "d9a979c437987ead"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
