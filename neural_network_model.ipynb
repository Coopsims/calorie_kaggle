{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Calories Prediction - Neural Network Model\n",
    "\n",
    "This notebook implements a Neural Network model to predict calories burned during workouts for the Kaggle Playground Series competition.\n"
   ],
   "id": "9bdc29dcf3f628c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Import Libraries\n",
   "id": "b0cea4e08deccc36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T00:36:23.872885Z",
     "start_time": "2025-05-09T00:36:23.829462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error, r2_score\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import multiprocessing\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42) if torch.cuda.is_available() else None\n",
    "\n",
    "# Set device - check for CUDA first, then MPS (for Mac M1/M2), then fall back to CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Metal Performance Shaders) device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device\")\n",
    "\n",
    "# Set up multiprocessing for CPU if needed\n",
    "num_workers = 0\n",
    "if device.type == 'cpu':\n",
    "    # Set multiprocessing start method (for Jupyter notebook compatibility)\n",
    "    try:\n",
    "        multiprocessing.set_start_method('spawn', force=True)\n",
    "        print(\"Using 'spawn' start method for multiprocessing\")\n",
    "    except RuntimeError:\n",
    "        print(\"Could not set start method to 'spawn' for multiprocessing\")\n",
    "\n",
    "    # Set number of workers for DataLoader\n",
    "    num_workers = multiprocessing.cpu_count() - 1  # Leave one CPU for system tasks\n",
    "    num_workers = max(0, num_workers)  # Ensure non-negative\n",
    "\n",
    "    # Enable multiprocessing for PyTorch\n",
    "    torch.set_num_threads(num_workers)\n",
    "    print(f\"Using {num_workers} workers for data loading and {torch.get_num_threads()} threads for PyTorch operations\")\n"
   ],
   "id": "d5763931fa549066",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Metal Performance Shaders) device\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Load and Explore Data\n",
   "id": "1fa3599c6fa61395"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T00:36:26.999122Z",
     "start_time": "2025-05-09T00:36:26.748747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the training and test data\n",
    "train_data = pd.read_csv('playground-series-s5e5/train.csv')\n",
    "test_data = pd.read_csv('playground-series-s5e5/test.csv')\n",
    "\n",
    "# Display basic information about the training data\n",
    "print(\"Training data shape:\", train_data.shape)\n",
    "train_data.head()\n"
   ],
   "id": "3ce3efb20292beb2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (750000, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   id     Sex  Age  Height  Weight  Duration  Heart_Rate  Body_Temp  Calories\n",
       "0   0    male   36   189.0    82.0      26.0       101.0       41.0     150.0\n",
       "1   1  female   64   163.0    60.0       8.0        85.0       39.7      34.0\n",
       "2   2  female   51   161.0    64.0       7.0        84.0       39.8      29.0\n",
       "3   3    male   20   192.0    90.0      25.0       105.0       40.7     140.0\n",
       "4   4  female   38   166.0    61.0      25.0       102.0       40.6     146.0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Heart_Rate</th>\n",
       "      <th>Body_Temp</th>\n",
       "      <th>Calories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>36</td>\n",
       "      <td>189.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>64</td>\n",
       "      <td>163.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>39.7</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>51</td>\n",
       "      <td>161.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>20</td>\n",
       "      <td>192.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>40.7</td>\n",
       "      <td>140.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>166.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>40.6</td>\n",
       "      <td>146.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T00:36:28.369553Z",
     "start_time": "2025-05-09T00:36:28.339214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in training data:\")\n",
    "print(train_data.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in test data:\")\n",
    "print(test_data.isnull().sum())\n"
   ],
   "id": "b6f70d6495c82334",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in training data:\n",
      "id            0\n",
      "Sex           0\n",
      "Age           0\n",
      "Height        0\n",
      "Weight        0\n",
      "Duration      0\n",
      "Heart_Rate    0\n",
      "Body_Temp     0\n",
      "Calories      0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in test data:\n",
      "id            0\n",
      "Sex           0\n",
      "Age           0\n",
      "Height        0\n",
      "Weight        0\n",
      "Duration      0\n",
      "Heart_Rate    0\n",
      "Body_Temp     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T00:36:29.907439Z",
     "start_time": "2025-05-09T00:36:29.795117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Statistical summary of the training data\n",
    "train_data.describe()\n"
   ],
   "id": "a0aec71a4010d24",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  id            Age         Height         Weight  \\\n",
       "count  750000.000000  750000.000000  750000.000000  750000.000000   \n",
       "mean   374999.500000      41.420404     174.697685      75.145668   \n",
       "std    216506.495284      15.175049      12.824496      13.982704   \n",
       "min         0.000000      20.000000     126.000000      36.000000   \n",
       "25%    187499.750000      28.000000     164.000000      63.000000   \n",
       "50%    374999.500000      40.000000     174.000000      74.000000   \n",
       "75%    562499.250000      52.000000     185.000000      87.000000   \n",
       "max    749999.000000      79.000000     222.000000     132.000000   \n",
       "\n",
       "            Duration     Heart_Rate      Body_Temp       Calories  \n",
       "count  750000.000000  750000.000000  750000.000000  750000.000000  \n",
       "mean       15.421015      95.483995      40.036253      88.282781  \n",
       "std         8.354095       9.449845       0.779875      62.395349  \n",
       "min         1.000000      67.000000      37.100000       1.000000  \n",
       "25%         8.000000      88.000000      39.600000      34.000000  \n",
       "50%        15.000000      95.000000      40.300000      77.000000  \n",
       "75%        23.000000     103.000000      40.700000     136.000000  \n",
       "max        30.000000     128.000000      41.500000     314.000000  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Heart_Rate</th>\n",
       "      <th>Body_Temp</th>\n",
       "      <th>Calories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>750000.000000</td>\n",
       "      <td>750000.000000</td>\n",
       "      <td>750000.000000</td>\n",
       "      <td>750000.000000</td>\n",
       "      <td>750000.000000</td>\n",
       "      <td>750000.000000</td>\n",
       "      <td>750000.000000</td>\n",
       "      <td>750000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>374999.500000</td>\n",
       "      <td>41.420404</td>\n",
       "      <td>174.697685</td>\n",
       "      <td>75.145668</td>\n",
       "      <td>15.421015</td>\n",
       "      <td>95.483995</td>\n",
       "      <td>40.036253</td>\n",
       "      <td>88.282781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>216506.495284</td>\n",
       "      <td>15.175049</td>\n",
       "      <td>12.824496</td>\n",
       "      <td>13.982704</td>\n",
       "      <td>8.354095</td>\n",
       "      <td>9.449845</td>\n",
       "      <td>0.779875</td>\n",
       "      <td>62.395349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>37.100000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>187499.750000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>39.600000</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>374999.500000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>40.300000</td>\n",
       "      <td>77.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>562499.250000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>40.700000</td>\n",
       "      <td>136.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>749999.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>41.500000</td>\n",
       "      <td>314.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Feature Engineering\n",
   "id": "baa8f38684bfb9ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T00:36:31.528266Z",
     "start_time": "2025-05-09T00:36:31.474588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a copy of the datasets for feature engineering\n",
    "train_fe = train_data.copy()\n",
    "test_fe = test_data.copy()\n",
    "\n",
    "# Calculate BMI (Body Mass Index)\n",
    "train_fe['BMI'] = train_fe['Weight'] / ((train_fe['Height'] / 100) ** 2)\n",
    "test_fe['BMI'] = test_fe['Weight'] / ((test_fe['Height'] / 100) ** 2)\n",
    "\n",
    "# Create interaction features\n",
    "train_fe['Duration_HeartRate'] = train_fe['Duration'] * train_fe['Heart_Rate']\n",
    "test_fe['Duration_HeartRate'] = test_fe['Duration'] * test_fe['Heart_Rate']\n",
    "\n",
    "train_fe['Weight_Duration'] = train_fe['Weight'] * train_fe['Duration']\n",
    "test_fe['Weight_Duration'] = test_fe['Weight'] * test_fe['Duration']\n",
    "\n",
    "# Convert Sex to numerical (0 for female, 1 for male)\n",
    "train_fe['Sex_num'] = train_fe['Sex'].map({'female': 0, 'male': 1})\n",
    "test_fe['Sex_num'] = test_fe['Sex'].map({'female': 0, 'male': 1})\n",
    "\n",
    "# Display the new features\n",
    "train_fe.head()\n"
   ],
   "id": "538d8f406260cc25",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   id     Sex  Age  Height  Weight  Duration  Heart_Rate  Body_Temp  Calories  \\\n",
       "0   0    male   36   189.0    82.0      26.0       101.0       41.0     150.0   \n",
       "1   1  female   64   163.0    60.0       8.0        85.0       39.7      34.0   \n",
       "2   2  female   51   161.0    64.0       7.0        84.0       39.8      29.0   \n",
       "3   3    male   20   192.0    90.0      25.0       105.0       40.7     140.0   \n",
       "4   4  female   38   166.0    61.0      25.0       102.0       40.6     146.0   \n",
       "\n",
       "         BMI  Duration_HeartRate  Weight_Duration  Sex_num  \n",
       "0  22.955684              2626.0           2132.0        1  \n",
       "1  22.582709               680.0            480.0        0  \n",
       "2  24.690405               588.0            448.0        0  \n",
       "3  24.414062              2625.0           2250.0        1  \n",
       "4  22.136740              2550.0           1525.0        0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Heart_Rate</th>\n",
       "      <th>Body_Temp</th>\n",
       "      <th>Calories</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Duration_HeartRate</th>\n",
       "      <th>Weight_Duration</th>\n",
       "      <th>Sex_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>36</td>\n",
       "      <td>189.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>22.955684</td>\n",
       "      <td>2626.0</td>\n",
       "      <td>2132.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>64</td>\n",
       "      <td>163.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>39.7</td>\n",
       "      <td>34.0</td>\n",
       "      <td>22.582709</td>\n",
       "      <td>680.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>51</td>\n",
       "      <td>161.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>29.0</td>\n",
       "      <td>24.690405</td>\n",
       "      <td>588.0</td>\n",
       "      <td>448.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>20</td>\n",
       "      <td>192.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>40.7</td>\n",
       "      <td>140.0</td>\n",
       "      <td>24.414062</td>\n",
       "      <td>2625.0</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>166.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>40.6</td>\n",
       "      <td>146.0</td>\n",
       "      <td>22.136740</td>\n",
       "      <td>2550.0</td>\n",
       "      <td>1525.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Prepare Data for Modeling\n",
   "id": "e7a403866f863b5b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T00:36:33.931385Z",
     "start_time": "2025-05-09T00:36:33.867139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define features and target\n",
    "features = ['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp', 'Sex_num', 'BMI', 'Duration_HeartRate', 'Weight_Duration']\n",
    "X = train_fe[features]\n",
    "y = train_fe['Calories']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n"
   ],
   "id": "83da1a7b505cb9e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (600000, 10)\n",
      "Validation set shape: (150000, 10)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T00:36:35.724924Z",
     "start_time": "2025-05-09T00:36:35.666854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Prepare test data\n",
    "X_test = test_fe[features]\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ],
   "id": "a275dcecbb24797c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Build a Basic Neural Network Model\n",
   "id": "774cf6a039d5ed51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T00:36:37.540908Z",
     "start_time": "2025-05-09T00:36:37.535387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a PyTorch neural network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers=[64, 32], activation='relu', \n",
    "                 dropout_rate=0.2, l1_reg=0.0, l2_reg=0.0):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # Set activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'elu':\n",
    "            self.activation = nn.ELU()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        else:\n",
    "            self.activation = nn.ReLU()  # Default to ReLU\n",
    "\n",
    "        # Create layers list\n",
    "        layers = []\n",
    "\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_dim, hidden_layers[0]))\n",
    "        layers.append(self.activation)\n",
    "        layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_layers) - 1):\n",
    "            layers.append(nn.Linear(hidden_layers[i], hidden_layers[i+1]))\n",
    "            layers.append(self.activation)\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_layers[-1], 1))\n",
    "\n",
    "        # Create sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "        # Store regularization parameters\n",
    "        self.l1_reg = l1_reg\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def l1_regularization(self):\n",
    "        l1_loss = 0\n",
    "        for param in self.parameters():\n",
    "            l1_loss += torch.sum(torch.abs(param))\n",
    "        return self.l1_reg * l1_loss\n",
    "\n",
    "    def l2_regularization(self):\n",
    "        l2_loss = 0\n",
    "        for param in self.parameters():\n",
    "            l2_loss += torch.sum(param.pow(2))\n",
    "        return self.l2_reg * l2_loss\n",
    "\n",
    "# Function to create and configure the model\n",
    "def create_model(input_dim, hidden_layers=[64, 32], activation='relu', \n",
    "                 learning_rate=0.001, dropout_rate=0.2, l1_reg=0.0, l2_reg=0.0):\n",
    "    # Create model\n",
    "    model = NeuralNetwork(\n",
    "        input_dim=input_dim,\n",
    "        hidden_layers=hidden_layers,\n",
    "        activation=activation,\n",
    "        dropout_rate=dropout_rate,\n",
    "        l1_reg=l1_reg,\n",
    "        l2_reg=l2_reg\n",
    "    ).to(device)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    return model, optimizer\n"
   ],
   "id": "6a29ec58d570aeb0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T00:36:42.735215Z",
     "start_time": "2025-05-09T00:36:40.888834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).to(device).reshape(-1, 1)\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val.values).to(device).reshape(-1, 1)\n",
    "\n",
    "# Create a basic neural network model\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "basic_model, basic_optimizer = create_model(input_dim)\n",
    "\n",
    "# Display model summary\n",
    "print(basic_model)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in basic_model.parameters())}\")\n"
   ],
   "id": "fa82bd0a07a45379",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (activation): ReLU()\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Total parameters: 2817\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T00:53:24.957221Z",
     "start_time": "2025-05-09T00:40:20.181197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a function for mean squared logarithmic error\n",
    "def msle_loss(pred, target):\n",
    "    # Ensure predictions are positive\n",
    "    pred = torch.clamp(pred, min=1e-6)\n",
    "    target = torch.clamp(target, min=1e-6)\n",
    "\n",
    "    # Calculate MSLE\n",
    "    return torch.mean(torch.pow(torch.log1p(pred) - torch.log1p(target), 2))\n",
    "\n",
    "# Define a function for training with early stopping\n",
    "def train_model(model, optimizer, X_train, y_train, X_val, y_val, \n",
    "                epochs=100, batch_size=32, patience=20, factor=0.2, min_lr=0.0001, \n",
    "                verbose=1):\n",
    "    \"\"\"\n",
    "    Train a PyTorch model with early stopping and learning rate reduction.\n",
    "\n",
    "    Parameters:\n",
    "    - model: PyTorch model to train\n",
    "    - optimizer: PyTorch optimizer\n",
    "    - X_train, y_train: Training data\n",
    "    - X_val, y_val: Validation data\n",
    "    - epochs: Maximum number of epochs to train\n",
    "    - batch_size: Batch size for training\n",
    "    - patience: Number of epochs with no improvement after which training will be stopped\n",
    "    - factor: Factor by which the learning rate will be reduced\n",
    "    - min_lr: Minimum learning rate\n",
    "    - verbose: Verbosity level (0=silent, 1=normal, 2=detailed)\n",
    "\n",
    "    Returns:\n",
    "    - model: Trained model\n",
    "    - history: Training history\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from datetime import timedelta\n",
    "    import numpy as np\n",
    "\n",
    "    # Initialize variables for early stopping and learning rate reduction\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_mses = []\n",
    "    val_mses = []\n",
    "\n",
    "    # Time tracking\n",
    "    start_time = time.time()\n",
    "    epoch_times = []\n",
    "\n",
    "    # Print training information\n",
    "    if verbose > 0:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Starting training with {epochs} epochs, batch size {batch_size}\")\n",
    "        print(f\"Device: {device}, Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "        print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n",
    "        print(f\"Learning rate: {optimizer.param_groups[0]['lr']}, Patience: {patience}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=num_workers if device.type == 'cpu' else 0,\n",
    "        pin_memory=(device.type != 'cpu')\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_mse = 0\n",
    "\n",
    "        # Batch progress tracking\n",
    "        if verbose > 1:\n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs}:\")\n",
    "            print(f\"{'='*50}\")\n",
    "\n",
    "        # Track batch progress\n",
    "        total_batches = len(train_loader)\n",
    "\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            # Forward pass\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = msle_loss(y_pred, y_batch)\n",
    "\n",
    "            # Add regularization if needed\n",
    "            if model.l1_reg > 0:\n",
    "                l1_loss = model.l1_regularization()\n",
    "                loss += l1_loss\n",
    "            if model.l2_reg > 0:\n",
    "                l2_loss = model.l2_regularization()\n",
    "                loss += l2_loss\n",
    "\n",
    "            # Calculate MSE for tracking\n",
    "            mse = torch.mean(torch.pow(y_pred - y_batch, 2))\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate batch loss\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "            train_mse += mse.item() * X_batch.size(0)\n",
    "\n",
    "            # Print batch progress\n",
    "            if verbose > 1 and (batch_idx % max(1, total_batches // 10) == 0 or batch_idx == total_batches - 1):\n",
    "                batch_loss = loss.item()\n",
    "                batch_mse = mse.item()\n",
    "                progress = (batch_idx + 1) / total_batches * 100\n",
    "                print(f\"Batch {batch_idx+1}/{total_batches} [{progress:.1f}%] - Loss: {batch_loss:.6f}, MSE: {batch_mse:.6f}\")\n",
    "\n",
    "        # Calculate average training loss\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_mse /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        train_mses.append(train_mse)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = model(X_val)\n",
    "            val_loss = msle_loss(y_val_pred, y_val).item()\n",
    "            val_mse = torch.mean(torch.pow(y_val_pred - y_val, 2)).item()\n",
    "\n",
    "            # Calculate RMSLE for more detailed reporting\n",
    "            y_val_np = y_val.cpu().numpy()\n",
    "            y_val_pred_np = y_val_pred.cpu().numpy().flatten()\n",
    "            y_val_pred_np = np.maximum(y_val_pred_np, 0)  # Ensure predictions are positive\n",
    "            val_rmsle = np.sqrt(mean_squared_log_error(y_val_np, y_val_pred_np))\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        val_mses.append(val_mse)\n",
    "\n",
    "        # Calculate epoch time\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_time = epoch_end_time - epoch_start_time\n",
    "        epoch_times.append(epoch_time)\n",
    "\n",
    "        # Calculate estimated time remaining\n",
    "        avg_epoch_time = np.mean(epoch_times)\n",
    "        remaining_epochs = epochs - (epoch + 1)\n",
    "        estimated_time_remaining = avg_epoch_time * remaining_epochs\n",
    "\n",
    "        # Print progress\n",
    "        if verbose > 0 and (epoch % max(1, epochs // 20) == 0 or epoch == epochs - 1):\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - {timedelta(seconds=int(epoch_time))} - ETA: {timedelta(seconds=int(estimated_time_remaining))}\")\n",
    "            print(f\"  Train Loss: {train_loss:.6f}, Train MSE: {train_mse:.6f}\")\n",
    "            print(f\"  Val Loss: {val_loss:.6f}, Val MSE: {val_mse:.6f}, Val RMSLE: {val_rmsle:.6f}\")\n",
    "\n",
    "            # Print current learning rate\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"  Learning rate: {current_lr:.6f}\")\n",
    "\n",
    "            # Print improvement information\n",
    "            if epoch > 0:\n",
    "                prev_val_loss = val_losses[-2]\n",
    "                loss_change = (prev_val_loss - val_loss) / prev_val_loss * 100\n",
    "                change_sign = \"↓\" if loss_change > 0 else \"↑\"\n",
    "                print(f\"  Validation loss change: {change_sign} {abs(loss_change):.2f}%\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            improvement = (best_val_loss - val_loss) / best_val_loss * 100\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(f\"  ✓ New best validation loss: {best_val_loss:.6f} (improved by {improvement:.2f}%)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if verbose > 0:\n",
    "                print(f\"  ✗ No improvement for {patience_counter}/{patience} epochs. Best: {best_val_loss:.6f}\")\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\n{'='*50}\")\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "                print(f\"{'='*50}\")\n",
    "                break\n",
    "\n",
    "        # Learning rate reduction\n",
    "        if patience_counter > 0 and patience_counter % 5 == 0:\n",
    "            old_lr = optimizer.param_groups[0]['lr']\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = max(param_group['lr'] * factor, min_lr)\n",
    "            new_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(f\"  → Reducing learning rate: {old_lr:.6f} → {new_lr:.6f}\")\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Calculate total training time\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training completed in {timedelta(seconds=int(total_time))}\")\n",
    "        print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "        print(f\"Total epochs: {len(train_losses)}/{epochs}\")\n",
    "        print(f\"Average epoch time: {timedelta(seconds=int(np.mean(epoch_times)))}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # Return training history\n",
    "    history = {\n",
    "        'loss': train_losses,\n",
    "        'val_loss': val_losses,\n",
    "        'mse': train_mses,\n",
    "        'val_mse': val_mses,\n",
    "        'epoch_times': epoch_times,\n",
    "        'total_time': total_time,\n",
    "        'best_val_loss': best_val_loss\n",
    "    }\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# Train the basic model\n",
    "basic_model, history = train_model(\n",
    "    basic_model, basic_optimizer,\n",
    "    X_train_tensor, y_train_tensor,\n",
    "    X_val_tensor, y_val_tensor,\n",
    "    epochs=100,\n",
    "    batch_size=1000,\n",
    "    verbose=2  # Set to 2 for more detailed output\n",
    ")\n"
   ],
   "id": "9ec2b7d1eb5adce5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Starting training with 100 epochs, batch size 1000\n",
      "Device: mps, Model parameters: 2817\n",
      "Training samples: 600000, Validation samples: 150000\n",
      "Learning rate: 0.001, Patience: 20\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Epoch 1/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.019510, MSE: 230.345383\n",
      "Batch 61/600 [10.2%] - Loss: 0.021878, MSE: 263.829834\n",
      "Batch 121/600 [20.2%] - Loss: 0.018556, MSE: 237.502106\n",
      "Batch 181/600 [30.2%] - Loss: 0.021451, MSE: 244.836761\n",
      "Batch 241/600 [40.2%] - Loss: 0.022569, MSE: 232.421295\n",
      "Batch 301/600 [50.2%] - Loss: 0.030044, MSE: 216.616135\n",
      "Batch 361/600 [60.2%] - Loss: 0.022076, MSE: 273.329132\n",
      "Batch 421/600 [70.2%] - Loss: 0.020186, MSE: 226.174316\n",
      "Batch 481/600 [80.2%] - Loss: 0.019562, MSE: 254.799820\n",
      "Batch 541/600 [90.2%] - Loss: 0.019227, MSE: 227.667755\n",
      "Batch 600/600 [100.0%] - Loss: 0.017921, MSE: 218.830795\n",
      "Epoch 1/100 - 0:00:12 - ETA: 0:19:50\n",
      "  Train Loss: 0.020995, Train MSE: 234.900398\n",
      "  Val Loss: 0.004555, Val MSE: 17.878750, Val RMSLE: 0.067491\n",
      "  Learning rate: 0.001000\n",
      "  ✓ New best validation loss: 0.004555 (improved by nan%)\n",
      "\n",
      "Epoch 2/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.021331, MSE: 240.144196\n",
      "Batch 61/600 [10.2%] - Loss: 0.020047, MSE: 204.834564\n",
      "Batch 121/600 [20.2%] - Loss: 0.021576, MSE: 260.769257\n",
      "Batch 181/600 [30.2%] - Loss: 0.018767, MSE: 215.083633\n",
      "Batch 241/600 [40.2%] - Loss: 0.023418, MSE: 254.728531\n",
      "Batch 301/600 [50.2%] - Loss: 0.021397, MSE: 252.649231\n",
      "Batch 361/600 [60.2%] - Loss: 0.023703, MSE: 246.771103\n",
      "Batch 421/600 [70.2%] - Loss: 0.020899, MSE: 250.654266\n",
      "Batch 481/600 [80.2%] - Loss: 0.019188, MSE: 214.632385\n",
      "Batch 541/600 [90.2%] - Loss: 0.021774, MSE: 252.500824\n",
      "Batch 600/600 [100.0%] - Loss: 0.021312, MSE: 251.012924\n",
      "  ✗ No improvement for 1/20 epochs. Best: 0.004555\n",
      "\n",
      "Epoch 3/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.020614, MSE: 236.609360\n",
      "Batch 61/600 [10.2%] - Loss: 0.021260, MSE: 253.588165\n",
      "Batch 121/600 [20.2%] - Loss: 0.018569, MSE: 213.576691\n",
      "Batch 181/600 [30.2%] - Loss: 0.021191, MSE: 246.860153\n",
      "Batch 241/600 [40.2%] - Loss: 0.019299, MSE: 200.739899\n",
      "Batch 301/600 [50.2%] - Loss: 0.019781, MSE: 227.539017\n",
      "Batch 361/600 [60.2%] - Loss: 0.021746, MSE: 257.398621\n",
      "Batch 421/600 [70.2%] - Loss: 0.018942, MSE: 206.320389\n",
      "Batch 481/600 [80.2%] - Loss: 0.021411, MSE: 221.062881\n",
      "Batch 541/600 [90.2%] - Loss: 0.020982, MSE: 242.102982\n",
      "Batch 600/600 [100.0%] - Loss: 0.018422, MSE: 243.679993\n",
      "  ✗ No improvement for 2/20 epochs. Best: 0.004555\n",
      "\n",
      "Epoch 4/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.024803, MSE: 234.497330\n",
      "Batch 61/600 [10.2%] - Loss: 0.022272, MSE: 237.328110\n",
      "Batch 121/600 [20.2%] - Loss: 0.021999, MSE: 234.212601\n",
      "Batch 181/600 [30.2%] - Loss: 0.019310, MSE: 228.049225\n",
      "Batch 241/600 [40.2%] - Loss: 0.022025, MSE: 251.063538\n",
      "Batch 301/600 [50.2%] - Loss: 0.020636, MSE: 238.485214\n",
      "Batch 361/600 [60.2%] - Loss: 0.020030, MSE: 242.042297\n",
      "Batch 421/600 [70.2%] - Loss: 0.021899, MSE: 217.279572\n",
      "Batch 481/600 [80.2%] - Loss: 0.019525, MSE: 206.763000\n",
      "Batch 541/600 [90.2%] - Loss: 0.017672, MSE: 213.850006\n",
      "Batch 600/600 [100.0%] - Loss: 0.019696, MSE: 239.868515\n",
      "  ✗ No improvement for 3/20 epochs. Best: 0.004555\n",
      "\n",
      "Epoch 5/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.019702, MSE: 198.941574\n",
      "Batch 61/600 [10.2%] - Loss: 0.019708, MSE: 219.439316\n",
      "Batch 121/600 [20.2%] - Loss: 0.018657, MSE: 217.031281\n",
      "Batch 181/600 [30.2%] - Loss: 0.021305, MSE: 252.524094\n",
      "Batch 241/600 [40.2%] - Loss: 0.020931, MSE: 205.109070\n",
      "Batch 301/600 [50.2%] - Loss: 0.021211, MSE: 214.119827\n",
      "Batch 361/600 [60.2%] - Loss: 0.020810, MSE: 228.625763\n",
      "Batch 421/600 [70.2%] - Loss: 0.020148, MSE: 229.555527\n",
      "Batch 481/600 [80.2%] - Loss: 0.023307, MSE: 243.599258\n",
      "Batch 541/600 [90.2%] - Loss: 0.019184, MSE: 210.811508\n",
      "Batch 600/600 [100.0%] - Loss: 0.019975, MSE: 228.679016\n",
      "  ✓ New best validation loss: 0.004555 (improved by 0.01%)\n",
      "\n",
      "Epoch 6/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.019912, MSE: 201.219757\n",
      "Batch 61/600 [10.2%] - Loss: 0.021296, MSE: 244.077545\n",
      "Batch 121/600 [20.2%] - Loss: 0.018223, MSE: 205.330017\n",
      "Batch 181/600 [30.2%] - Loss: 0.023016, MSE: 252.183075\n",
      "Batch 241/600 [40.2%] - Loss: 0.020306, MSE: 205.507324\n",
      "Batch 301/600 [50.2%] - Loss: 0.019093, MSE: 229.250778\n",
      "Batch 361/600 [60.2%] - Loss: 0.019635, MSE: 227.100281\n",
      "Batch 421/600 [70.2%] - Loss: 0.021151, MSE: 234.948624\n",
      "Batch 481/600 [80.2%] - Loss: 0.017201, MSE: 206.045609\n",
      "Batch 541/600 [90.2%] - Loss: 0.019250, MSE: 219.872910\n",
      "Batch 600/600 [100.0%] - Loss: 0.019224, MSE: 219.127731\n",
      "Epoch 6/100 - 0:00:11 - ETA: 0:18:21\n",
      "  Train Loss: 0.020464, Train MSE: 229.542655\n",
      "  Val Loss: 0.004643, Val MSE: 18.242666, Val RMSLE: 0.068142\n",
      "  Learning rate: 0.001000\n",
      "  Validation loss change: ↑ 1.95%\n",
      "  ✗ No improvement for 1/20 epochs. Best: 0.004555\n",
      "\n",
      "Epoch 7/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.020039, MSE: 239.778717\n",
      "Batch 61/600 [10.2%] - Loss: 0.020168, MSE: 221.808472\n",
      "Batch 121/600 [20.2%] - Loss: 0.022528, MSE: 216.276382\n",
      "Batch 181/600 [30.2%] - Loss: 0.019845, MSE: 231.323914\n",
      "Batch 241/600 [40.2%] - Loss: 0.020288, MSE: 201.879791\n",
      "Batch 301/600 [50.2%] - Loss: 0.021481, MSE: 243.277817\n",
      "Batch 361/600 [60.2%] - Loss: 0.019122, MSE: 264.670074\n",
      "Batch 421/600 [70.2%] - Loss: 0.019554, MSE: 225.329758\n",
      "Batch 481/600 [80.2%] - Loss: 0.018006, MSE: 196.050735\n",
      "Batch 541/600 [90.2%] - Loss: 0.021360, MSE: 244.130295\n",
      "Batch 600/600 [100.0%] - Loss: 0.020790, MSE: 249.761368\n",
      "  ✗ No improvement for 2/20 epochs. Best: 0.004555\n",
      "\n",
      "Epoch 8/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.020045, MSE: 201.518570\n",
      "Batch 61/600 [10.2%] - Loss: 0.022113, MSE: 227.243347\n",
      "Batch 121/600 [20.2%] - Loss: 0.020402, MSE: 210.528580\n",
      "Batch 181/600 [30.2%] - Loss: 0.019398, MSE: 243.382889\n",
      "Batch 241/600 [40.2%] - Loss: 0.026892, MSE: 293.201935\n",
      "Batch 301/600 [50.2%] - Loss: 0.021928, MSE: 218.916016\n",
      "Batch 361/600 [60.2%] - Loss: 0.018400, MSE: 235.597122\n",
      "Batch 421/600 [70.2%] - Loss: 0.021307, MSE: 255.847290\n",
      "Batch 481/600 [80.2%] - Loss: 0.017877, MSE: 207.783447\n",
      "Batch 541/600 [90.2%] - Loss: 0.018596, MSE: 200.218979\n",
      "Batch 600/600 [100.0%] - Loss: 0.020388, MSE: 215.350174\n",
      "  ✓ New best validation loss: 0.004554 (improved by 0.01%)\n",
      "\n",
      "Epoch 9/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.021547, MSE: 259.308929\n",
      "Batch 61/600 [10.2%] - Loss: 0.023424, MSE: 235.081360\n",
      "Batch 121/600 [20.2%] - Loss: 0.019145, MSE: 221.899445\n",
      "Batch 181/600 [30.2%] - Loss: 0.023470, MSE: 252.376083\n",
      "Batch 241/600 [40.2%] - Loss: 0.019597, MSE: 250.910934\n",
      "Batch 301/600 [50.2%] - Loss: 0.019788, MSE: 232.464233\n",
      "Batch 361/600 [60.2%] - Loss: 0.018881, MSE: 213.165695\n",
      "Batch 421/600 [70.2%] - Loss: 0.021050, MSE: 230.734161\n",
      "Batch 481/600 [80.2%] - Loss: 0.019935, MSE: 202.288345\n",
      "Batch 541/600 [90.2%] - Loss: 0.019013, MSE: 214.107697\n",
      "Batch 600/600 [100.0%] - Loss: 0.019874, MSE: 208.588943\n",
      "  ✗ No improvement for 1/20 epochs. Best: 0.004554\n",
      "\n",
      "Epoch 10/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.020455, MSE: 222.852570\n",
      "Batch 61/600 [10.2%] - Loss: 0.020550, MSE: 244.649521\n",
      "Batch 121/600 [20.2%] - Loss: 0.021032, MSE: 215.592453\n",
      "Batch 181/600 [30.2%] - Loss: 0.018917, MSE: 216.175949\n",
      "Batch 241/600 [40.2%] - Loss: 0.019661, MSE: 213.099121\n",
      "Batch 301/600 [50.2%] - Loss: 0.019034, MSE: 219.995819\n",
      "Batch 361/600 [60.2%] - Loss: 0.019642, MSE: 233.756454\n",
      "Batch 421/600 [70.2%] - Loss: 0.020849, MSE: 237.198730\n",
      "Batch 481/600 [80.2%] - Loss: 0.020567, MSE: 233.855972\n",
      "Batch 541/600 [90.2%] - Loss: 0.019493, MSE: 210.957733\n",
      "Batch 600/600 [100.0%] - Loss: 0.019424, MSE: 231.697067\n",
      "  ✗ No improvement for 2/20 epochs. Best: 0.004554\n",
      "\n",
      "Epoch 11/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.020066, MSE: 233.329254\n",
      "Batch 61/600 [10.2%] - Loss: 0.018987, MSE: 228.581100\n",
      "Batch 121/600 [20.2%] - Loss: 0.023865, MSE: 221.590347\n",
      "Batch 181/600 [30.2%] - Loss: 0.020091, MSE: 225.219864\n",
      "Batch 241/600 [40.2%] - Loss: 0.017787, MSE: 227.338226\n",
      "Batch 301/600 [50.2%] - Loss: 0.017865, MSE: 198.257538\n",
      "Batch 361/600 [60.2%] - Loss: 0.019266, MSE: 220.535843\n",
      "Batch 421/600 [70.2%] - Loss: 0.021026, MSE: 206.403946\n",
      "Batch 481/600 [80.2%] - Loss: 0.018457, MSE: 200.107864\n",
      "Batch 541/600 [90.2%] - Loss: 0.021011, MSE: 230.228302\n",
      "Batch 600/600 [100.0%] - Loss: 0.019956, MSE: 235.437576\n",
      "Epoch 11/100 - 0:00:11 - ETA: 0:17:21\n",
      "  Train Loss: 0.020140, Train MSE: 227.216104\n",
      "  Val Loss: 0.004687, Val MSE: 16.740557, Val RMSLE: 0.068460\n",
      "  Learning rate: 0.001000\n",
      "  Validation loss change: ↓ 0.45%\n",
      "  ✗ No improvement for 3/20 epochs. Best: 0.004554\n",
      "\n",
      "Epoch 12/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.020951, MSE: 237.837967\n",
      "Batch 61/600 [10.2%] - Loss: 0.018924, MSE: 227.088547\n",
      "Batch 121/600 [20.2%] - Loss: 0.020051, MSE: 232.777283\n",
      "Batch 181/600 [30.2%] - Loss: 0.020035, MSE: 234.595886\n",
      "Batch 241/600 [40.2%] - Loss: 0.018766, MSE: 221.601776\n",
      "Batch 301/600 [50.2%] - Loss: 0.018876, MSE: 215.893738\n",
      "Batch 361/600 [60.2%] - Loss: 0.020084, MSE: 236.829239\n",
      "Batch 421/600 [70.2%] - Loss: 0.018787, MSE: 233.755798\n",
      "Batch 481/600 [80.2%] - Loss: 0.019674, MSE: 243.909698\n",
      "Batch 541/600 [90.2%] - Loss: 0.018691, MSE: 210.880600\n",
      "Batch 600/600 [100.0%] - Loss: 0.019733, MSE: 223.352173\n",
      "  ✗ No improvement for 4/20 epochs. Best: 0.004554\n",
      "\n",
      "Epoch 13/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.017686, MSE: 225.288208\n",
      "Batch 61/600 [10.2%] - Loss: 0.018702, MSE: 221.449295\n",
      "Batch 121/600 [20.2%] - Loss: 0.018686, MSE: 211.693161\n",
      "Batch 181/600 [30.2%] - Loss: 0.018768, MSE: 201.064392\n",
      "Batch 241/600 [40.2%] - Loss: 0.019561, MSE: 185.124542\n",
      "Batch 301/600 [50.2%] - Loss: 0.018860, MSE: 189.500763\n",
      "Batch 361/600 [60.2%] - Loss: 0.023722, MSE: 197.534149\n",
      "Batch 421/600 [70.2%] - Loss: 0.027183, MSE: 257.357758\n",
      "Batch 481/600 [80.2%] - Loss: 0.020158, MSE: 222.477173\n",
      "Batch 541/600 [90.2%] - Loss: 0.020133, MSE: 222.726089\n",
      "Batch 600/600 [100.0%] - Loss: 0.020004, MSE: 240.231323\n",
      "  ✗ No improvement for 5/20 epochs. Best: 0.004554\n",
      "  → Reducing learning rate: 0.001000 → 0.000200\n",
      "\n",
      "Epoch 14/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.021067, MSE: 241.108261\n",
      "Batch 61/600 [10.2%] - Loss: 0.019038, MSE: 226.309494\n",
      "Batch 121/600 [20.2%] - Loss: 0.024658, MSE: 247.106384\n",
      "Batch 181/600 [30.2%] - Loss: 0.026226, MSE: 273.179260\n",
      "Batch 241/600 [40.2%] - Loss: 0.020410, MSE: 232.056610\n",
      "Batch 301/600 [50.2%] - Loss: 0.017895, MSE: 208.507706\n",
      "Batch 361/600 [60.2%] - Loss: 0.018601, MSE: 232.068344\n",
      "Batch 421/600 [70.2%] - Loss: 0.019355, MSE: 260.572632\n",
      "Batch 481/600 [80.2%] - Loss: 0.017020, MSE: 181.142487\n",
      "Batch 541/600 [90.2%] - Loss: 0.020047, MSE: 185.411285\n",
      "Batch 600/600 [100.0%] - Loss: 0.019218, MSE: 209.883102\n",
      "  ✗ No improvement for 6/20 epochs. Best: 0.004554\n",
      "\n",
      "Epoch 15/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.018263, MSE: 217.160858\n",
      "Batch 61/600 [10.2%] - Loss: 0.018832, MSE: 236.065857\n",
      "Batch 121/600 [20.2%] - Loss: 0.017875, MSE: 216.051514\n",
      "Batch 181/600 [30.2%] - Loss: 0.020785, MSE: 256.394043\n",
      "Batch 241/600 [40.2%] - Loss: 0.019942, MSE: 245.343445\n",
      "Batch 301/600 [50.2%] - Loss: 0.017681, MSE: 198.618240\n",
      "Batch 361/600 [60.2%] - Loss: 0.021421, MSE: 237.170288\n",
      "Batch 421/600 [70.2%] - Loss: 0.019464, MSE: 258.980377\n",
      "Batch 481/600 [80.2%] - Loss: 0.018755, MSE: 219.795410\n",
      "Batch 541/600 [90.2%] - Loss: 0.022880, MSE: 251.650803\n",
      "Batch 600/600 [100.0%] - Loss: 0.020622, MSE: 217.931702\n",
      "  ✓ New best validation loss: 0.004489 (improved by 1.42%)\n",
      "\n",
      "Epoch 16/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.018761, MSE: 212.738525\n",
      "Batch 61/600 [10.2%] - Loss: 0.019983, MSE: 222.074173\n",
      "Batch 121/600 [20.2%] - Loss: 0.019747, MSE: 232.319260\n",
      "Batch 181/600 [30.2%] - Loss: 0.024432, MSE: 259.526398\n",
      "Batch 241/600 [40.2%] - Loss: 0.018065, MSE: 208.989868\n",
      "Batch 301/600 [50.2%] - Loss: 0.020126, MSE: 232.623367\n",
      "Batch 361/600 [60.2%] - Loss: 0.018645, MSE: 200.595200\n",
      "Batch 421/600 [70.2%] - Loss: 0.019325, MSE: 191.001556\n",
      "Batch 481/600 [80.2%] - Loss: 0.017929, MSE: 219.845673\n",
      "Batch 541/600 [90.2%] - Loss: 0.022073, MSE: 232.124023\n",
      "Batch 600/600 [100.0%] - Loss: 0.019110, MSE: 214.030899\n",
      "Epoch 16/100 - 0:00:11 - ETA: 0:16:18\n",
      "  Train Loss: 0.019799, Train MSE: 224.332675\n",
      "  Val Loss: 0.004628, Val MSE: 16.506855, Val RMSLE: 0.068029\n",
      "  Learning rate: 0.000200\n",
      "  Validation loss change: ↑ 3.09%\n",
      "  ✗ No improvement for 1/20 epochs. Best: 0.004489\n",
      "\n",
      "Epoch 17/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.027293, MSE: 226.107269\n",
      "Batch 61/600 [10.2%] - Loss: 0.022492, MSE: 260.257385\n",
      "Batch 121/600 [20.2%] - Loss: 0.022612, MSE: 245.662735\n",
      "Batch 181/600 [30.2%] - Loss: 0.019310, MSE: 223.858856\n",
      "Batch 241/600 [40.2%] - Loss: 0.020135, MSE: 219.677643\n",
      "Batch 301/600 [50.2%] - Loss: 0.019526, MSE: 252.163193\n",
      "Batch 361/600 [60.2%] - Loss: 0.020339, MSE: 202.000137\n",
      "Batch 421/600 [70.2%] - Loss: 0.019386, MSE: 226.772339\n",
      "Batch 481/600 [80.2%] - Loss: 0.017325, MSE: 219.760880\n",
      "Batch 541/600 [90.2%] - Loss: 0.017234, MSE: 223.762360\n",
      "Batch 600/600 [100.0%] - Loss: 0.018123, MSE: 180.218536\n",
      "  ✗ No improvement for 2/20 epochs. Best: 0.004489\n",
      "\n",
      "Epoch 18/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.018646, MSE: 238.642548\n",
      "Batch 61/600 [10.2%] - Loss: 0.019362, MSE: 221.499603\n",
      "Batch 121/600 [20.2%] - Loss: 0.022322, MSE: 256.630554\n",
      "Batch 181/600 [30.2%] - Loss: 0.022322, MSE: 239.736069\n",
      "Batch 241/600 [40.2%] - Loss: 0.021334, MSE: 233.016418\n",
      "Batch 301/600 [50.2%] - Loss: 0.021628, MSE: 224.749069\n",
      "Batch 361/600 [60.2%] - Loss: 0.018704, MSE: 215.500534\n",
      "Batch 421/600 [70.2%] - Loss: 0.021603, MSE: 278.058167\n",
      "Batch 481/600 [80.2%] - Loss: 0.018672, MSE: 186.128464\n",
      "Batch 541/600 [90.2%] - Loss: 0.019048, MSE: 236.290756\n",
      "Batch 600/600 [100.0%] - Loss: 0.019419, MSE: 243.668747\n",
      "  ✗ No improvement for 3/20 epochs. Best: 0.004489\n",
      "\n",
      "Epoch 19/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.018690, MSE: 240.454254\n",
      "Batch 61/600 [10.2%] - Loss: 0.026285, MSE: 249.857788\n",
      "Batch 121/600 [20.2%] - Loss: 0.018324, MSE: 209.730072\n",
      "Batch 181/600 [30.2%] - Loss: 0.017620, MSE: 210.871445\n",
      "Batch 241/600 [40.2%] - Loss: 0.018153, MSE: 200.790009\n",
      "Batch 301/600 [50.2%] - Loss: 0.019981, MSE: 244.364288\n",
      "Batch 361/600 [60.2%] - Loss: 0.021166, MSE: 227.420288\n",
      "Batch 421/600 [70.2%] - Loss: 0.019092, MSE: 239.327011\n",
      "Batch 481/600 [80.2%] - Loss: 0.020196, MSE: 198.487946\n",
      "Batch 541/600 [90.2%] - Loss: 0.019018, MSE: 205.760040\n",
      "Batch 600/600 [100.0%] - Loss: 0.018082, MSE: 186.832932\n",
      "  ✗ No improvement for 4/20 epochs. Best: 0.004489\n",
      "\n",
      "Epoch 20/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.017802, MSE: 206.412018\n",
      "Batch 61/600 [10.2%] - Loss: 0.019713, MSE: 216.496689\n",
      "Batch 121/600 [20.2%] - Loss: 0.022013, MSE: 228.740723\n",
      "Batch 181/600 [30.2%] - Loss: 0.020534, MSE: 228.178360\n",
      "Batch 241/600 [40.2%] - Loss: 0.019113, MSE: 223.981415\n",
      "Batch 301/600 [50.2%] - Loss: 0.018840, MSE: 229.861435\n",
      "Batch 361/600 [60.2%] - Loss: 0.025476, MSE: 254.256363\n",
      "Batch 421/600 [70.2%] - Loss: 0.018735, MSE: 211.946259\n",
      "Batch 481/600 [80.2%] - Loss: 0.018875, MSE: 229.053513\n",
      "Batch 541/600 [90.2%] - Loss: 0.018049, MSE: 209.631195\n",
      "Batch 600/600 [100.0%] - Loss: 0.019193, MSE: 222.693161\n",
      "  ✗ No improvement for 5/20 epochs. Best: 0.004489\n",
      "  → Reducing learning rate: 0.000200 → 0.000100\n",
      "\n",
      "Epoch 21/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.020809, MSE: 224.168762\n",
      "Batch 61/600 [10.2%] - Loss: 0.019967, MSE: 222.164230\n",
      "Batch 121/600 [20.2%] - Loss: 0.019387, MSE: 235.765732\n",
      "Batch 181/600 [30.2%] - Loss: 0.020872, MSE: 213.391510\n",
      "Batch 241/600 [40.2%] - Loss: 0.020235, MSE: 225.917465\n",
      "Batch 301/600 [50.2%] - Loss: 0.019455, MSE: 222.509048\n",
      "Batch 361/600 [60.2%] - Loss: 0.020646, MSE: 232.855209\n",
      "Batch 421/600 [70.2%] - Loss: 0.023059, MSE: 247.583145\n",
      "Batch 481/600 [80.2%] - Loss: 0.019058, MSE: 260.941925\n",
      "Batch 541/600 [90.2%] - Loss: 0.020806, MSE: 258.280334\n",
      "Batch 600/600 [100.0%] - Loss: 0.020553, MSE: 235.739655\n",
      "Epoch 21/100 - 0:00:11 - ETA: 0:15:22\n",
      "  Train Loss: 0.019696, Train MSE: 223.485759\n",
      "  Val Loss: 0.004509, Val MSE: 16.598595, Val RMSLE: 0.067149\n",
      "  Learning rate: 0.000100\n",
      "  Validation loss change: ↓ 2.47%\n",
      "  ✗ No improvement for 6/20 epochs. Best: 0.004489\n",
      "\n",
      "Epoch 22/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.018608, MSE: 215.894852\n",
      "Batch 61/600 [10.2%] - Loss: 0.019340, MSE: 207.392319\n",
      "Batch 121/600 [20.2%] - Loss: 0.018428, MSE: 225.575333\n",
      "Batch 181/600 [30.2%] - Loss: 0.018453, MSE: 215.093109\n",
      "Batch 241/600 [40.2%] - Loss: 0.021106, MSE: 248.022598\n",
      "Batch 301/600 [50.2%] - Loss: 0.020033, MSE: 219.509491\n",
      "Batch 361/600 [60.2%] - Loss: 0.019802, MSE: 218.385590\n",
      "Batch 421/600 [70.2%] - Loss: 0.019833, MSE: 241.526443\n",
      "Batch 481/600 [80.2%] - Loss: 0.016447, MSE: 196.771133\n",
      "Batch 541/600 [90.2%] - Loss: 0.019532, MSE: 203.483170\n",
      "Batch 600/600 [100.0%] - Loss: 0.018727, MSE: 230.390762\n",
      "  ✗ No improvement for 7/20 epochs. Best: 0.004489\n",
      "\n",
      "Epoch 23/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.021775, MSE: 229.189255\n",
      "Batch 61/600 [10.2%] - Loss: 0.017806, MSE: 181.709930\n",
      "Batch 121/600 [20.2%] - Loss: 0.021058, MSE: 217.096985\n",
      "Batch 181/600 [30.2%] - Loss: 0.018511, MSE: 213.810257\n",
      "Batch 241/600 [40.2%] - Loss: 0.019352, MSE: 238.921295\n",
      "Batch 301/600 [50.2%] - Loss: 0.020224, MSE: 229.422119\n",
      "Batch 361/600 [60.2%] - Loss: 0.018076, MSE: 199.553909\n",
      "Batch 421/600 [70.2%] - Loss: 0.019051, MSE: 197.741821\n",
      "Batch 481/600 [80.2%] - Loss: 0.018720, MSE: 231.574493\n",
      "Batch 541/600 [90.2%] - Loss: 0.020977, MSE: 231.711761\n",
      "Batch 600/600 [100.0%] - Loss: 0.018620, MSE: 207.700882\n",
      "  ✗ No improvement for 8/20 epochs. Best: 0.004489\n",
      "\n",
      "Epoch 24/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.020454, MSE: 218.211914\n",
      "Batch 61/600 [10.2%] - Loss: 0.019895, MSE: 242.943390\n",
      "Batch 121/600 [20.2%] - Loss: 0.018159, MSE: 227.185394\n",
      "Batch 181/600 [30.2%] - Loss: 0.018257, MSE: 210.178757\n",
      "Batch 241/600 [40.2%] - Loss: 0.018702, MSE: 219.550323\n",
      "Batch 301/600 [50.2%] - Loss: 0.022762, MSE: 239.671829\n",
      "Batch 361/600 [60.2%] - Loss: 0.017727, MSE: 210.243317\n",
      "Batch 421/600 [70.2%] - Loss: 0.020166, MSE: 218.991699\n",
      "Batch 481/600 [80.2%] - Loss: 0.021401, MSE: 234.563736\n",
      "Batch 541/600 [90.2%] - Loss: 0.019892, MSE: 199.068680\n",
      "Batch 600/600 [100.0%] - Loss: 0.018427, MSE: 224.378815\n",
      "  ✗ No improvement for 9/20 epochs. Best: 0.004489\n",
      "\n",
      "Epoch 25/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.018407, MSE: 218.913712\n",
      "Batch 61/600 [10.2%] - Loss: 0.020780, MSE: 237.019974\n",
      "Batch 121/600 [20.2%] - Loss: 0.019866, MSE: 232.131454\n",
      "Batch 181/600 [30.2%] - Loss: 0.018877, MSE: 235.485947\n",
      "Batch 241/600 [40.2%] - Loss: 0.019136, MSE: 222.270599\n",
      "Batch 301/600 [50.2%] - Loss: 0.018876, MSE: 252.841080\n",
      "Batch 361/600 [60.2%] - Loss: 0.027588, MSE: 247.317429\n",
      "Batch 421/600 [70.2%] - Loss: 0.021178, MSE: 244.805313\n",
      "Batch 481/600 [80.2%] - Loss: 0.020132, MSE: 199.513977\n",
      "Batch 541/600 [90.2%] - Loss: 0.019470, MSE: 222.276321\n",
      "Batch 600/600 [100.0%] - Loss: 0.019575, MSE: 237.714462\n",
      "  ✗ No improvement for 10/20 epochs. Best: 0.004489\n",
      "  → Reducing learning rate: 0.000100 → 0.000100\n",
      "\n",
      "Epoch 26/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.024344, MSE: 227.124573\n",
      "Batch 61/600 [10.2%] - Loss: 0.020833, MSE: 257.744232\n",
      "Batch 121/600 [20.2%] - Loss: 0.020139, MSE: 225.755371\n",
      "Batch 181/600 [30.2%] - Loss: 0.019629, MSE: 215.499222\n",
      "Batch 241/600 [40.2%] - Loss: 0.019896, MSE: 248.851517\n",
      "Batch 301/600 [50.2%] - Loss: 0.019710, MSE: 224.164719\n",
      "Batch 361/600 [60.2%] - Loss: 0.019499, MSE: 236.217010\n",
      "Batch 421/600 [70.2%] - Loss: 0.018272, MSE: 197.294815\n",
      "Batch 481/600 [80.2%] - Loss: 0.019232, MSE: 210.071289\n",
      "Batch 541/600 [90.2%] - Loss: 0.017931, MSE: 211.928574\n",
      "Batch 600/600 [100.0%] - Loss: 0.017965, MSE: 198.889648\n",
      "Epoch 26/100 - 0:00:11 - ETA: 0:14:22\n",
      "  Train Loss: 0.019693, Train MSE: 222.993730\n",
      "  Val Loss: 0.004490, Val MSE: 16.415871, Val RMSLE: 0.067007\n",
      "  Learning rate: 0.000100\n",
      "  Validation loss change: ↓ 1.32%\n",
      "  ✗ No improvement for 11/20 epochs. Best: 0.004489\n",
      "\n",
      "Epoch 27/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.022678, MSE: 218.318451\n",
      "Batch 61/600 [10.2%] - Loss: 0.020561, MSE: 259.718353\n",
      "Batch 121/600 [20.2%] - Loss: 0.022148, MSE: 227.695938\n",
      "Batch 181/600 [30.2%] - Loss: 0.020131, MSE: 219.561462\n",
      "Batch 241/600 [40.2%] - Loss: 0.019018, MSE: 233.052139\n",
      "Batch 301/600 [50.2%] - Loss: 0.017767, MSE: 199.958130\n",
      "Batch 361/600 [60.2%] - Loss: 0.019532, MSE: 213.088730\n",
      "Batch 421/600 [70.2%] - Loss: 0.021181, MSE: 239.877319\n",
      "Batch 481/600 [80.2%] - Loss: 0.016969, MSE: 212.016113\n",
      "Batch 541/600 [90.2%] - Loss: 0.017833, MSE: 220.718384\n",
      "Batch 600/600 [100.0%] - Loss: 0.018808, MSE: 232.906067\n",
      "  ✗ No improvement for 12/20 epochs. Best: 0.004489\n",
      "\n",
      "Epoch 28/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.019522, MSE: 210.380798\n",
      "Batch 61/600 [10.2%] - Loss: 0.019477, MSE: 214.387711\n",
      "Batch 121/600 [20.2%] - Loss: 0.020392, MSE: 207.085236\n",
      "Batch 181/600 [30.2%] - Loss: 0.019726, MSE: 230.487564\n",
      "Batch 241/600 [40.2%] - Loss: 0.020780, MSE: 220.949234\n",
      "Batch 301/600 [50.2%] - Loss: 0.019959, MSE: 237.028976\n",
      "Batch 361/600 [60.2%] - Loss: 0.020524, MSE: 247.133438\n",
      "Batch 421/600 [70.2%] - Loss: 0.021552, MSE: 212.535034\n",
      "Batch 481/600 [80.2%] - Loss: 0.021602, MSE: 230.810684\n",
      "Batch 541/600 [90.2%] - Loss: 0.020166, MSE: 235.291107\n",
      "Batch 600/600 [100.0%] - Loss: 0.019037, MSE: 188.673096\n",
      "  ✗ No improvement for 13/20 epochs. Best: 0.004489\n",
      "\n",
      "Epoch 29/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.018716, MSE: 237.132141\n",
      "Batch 61/600 [10.2%] - Loss: 0.021549, MSE: 240.543793\n",
      "Batch 121/600 [20.2%] - Loss: 0.019694, MSE: 199.488388\n",
      "Batch 181/600 [30.2%] - Loss: 0.019701, MSE: 231.162567\n",
      "Batch 241/600 [40.2%] - Loss: 0.019326, MSE: 207.891159\n",
      "Batch 301/600 [50.2%] - Loss: 0.020592, MSE: 259.990051\n",
      "Batch 361/600 [60.2%] - Loss: 0.019001, MSE: 210.901489\n",
      "Batch 421/600 [70.2%] - Loss: 0.019822, MSE: 213.455505\n",
      "Batch 481/600 [80.2%] - Loss: 0.020126, MSE: 215.684982\n",
      "Batch 541/600 [90.2%] - Loss: 0.018787, MSE: 224.510117\n",
      "Batch 600/600 [100.0%] - Loss: 0.021109, MSE: 218.200363\n",
      "  ✓ New best validation loss: 0.004456 (improved by 0.75%)\n",
      "\n",
      "Epoch 30/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.023590, MSE: 227.230545\n",
      "Batch 61/600 [10.2%] - Loss: 0.018320, MSE: 221.708542\n",
      "Batch 121/600 [20.2%] - Loss: 0.017287, MSE: 207.749283\n",
      "Batch 181/600 [30.2%] - Loss: 0.019266, MSE: 245.100616\n",
      "Batch 241/600 [40.2%] - Loss: 0.019338, MSE: 227.209000\n",
      "Batch 301/600 [50.2%] - Loss: 0.018860, MSE: 223.114807\n",
      "Batch 361/600 [60.2%] - Loss: 0.019317, MSE: 215.210297\n",
      "Batch 421/600 [70.2%] - Loss: 0.018750, MSE: 200.912201\n",
      "Batch 481/600 [80.2%] - Loss: 0.019785, MSE: 227.768066\n",
      "Batch 541/600 [90.2%] - Loss: 0.018556, MSE: 204.027023\n",
      "Batch 600/600 [100.0%] - Loss: 0.019621, MSE: 216.561874\n",
      "  ✗ No improvement for 1/20 epochs. Best: 0.004456\n",
      "\n",
      "Epoch 31/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.020334, MSE: 234.574966\n",
      "Batch 61/600 [10.2%] - Loss: 0.018651, MSE: 224.384354\n",
      "Batch 121/600 [20.2%] - Loss: 0.018307, MSE: 185.210693\n",
      "Batch 181/600 [30.2%] - Loss: 0.019607, MSE: 214.549103\n",
      "Batch 241/600 [40.2%] - Loss: 0.020974, MSE: 204.508926\n",
      "Batch 301/600 [50.2%] - Loss: 0.018948, MSE: 225.036026\n",
      "Batch 361/600 [60.2%] - Loss: 0.018214, MSE: 250.119690\n",
      "Batch 421/600 [70.2%] - Loss: 0.019321, MSE: 217.398819\n",
      "Batch 481/600 [80.2%] - Loss: 0.019727, MSE: 203.717392\n",
      "Batch 541/600 [90.2%] - Loss: 0.028184, MSE: 234.425018\n",
      "Batch 600/600 [100.0%] - Loss: 0.019033, MSE: 211.633072\n",
      "Epoch 31/100 - 0:00:11 - ETA: 0:13:24\n",
      "  Train Loss: 0.019689, Train MSE: 223.859384\n",
      "  Val Loss: 0.004520, Val MSE: 16.460258, Val RMSLE: 0.067231\n",
      "  Learning rate: 0.000100\n",
      "  Validation loss change: ↓ 0.47%\n",
      "  ✗ No improvement for 2/20 epochs. Best: 0.004456\n",
      "\n",
      "Epoch 32/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.018557, MSE: 239.550659\n",
      "Batch 61/600 [10.2%] - Loss: 0.018161, MSE: 181.026917\n",
      "Batch 121/600 [20.2%] - Loss: 0.020447, MSE: 219.047897\n",
      "Batch 181/600 [30.2%] - Loss: 0.018389, MSE: 231.973572\n",
      "Batch 241/600 [40.2%] - Loss: 0.018373, MSE: 234.449615\n",
      "Batch 301/600 [50.2%] - Loss: 0.020407, MSE: 233.855301\n",
      "Batch 361/600 [60.2%] - Loss: 0.021837, MSE: 232.506851\n",
      "Batch 421/600 [70.2%] - Loss: 0.020199, MSE: 214.962219\n",
      "Batch 481/600 [80.2%] - Loss: 0.017803, MSE: 211.359299\n",
      "Batch 541/600 [90.2%] - Loss: 0.019254, MSE: 230.229950\n",
      "Batch 600/600 [100.0%] - Loss: 0.018129, MSE: 212.973297\n",
      "  ✗ No improvement for 3/20 epochs. Best: 0.004456\n",
      "\n",
      "Epoch 33/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.020236, MSE: 230.231979\n",
      "Batch 61/600 [10.2%] - Loss: 0.018308, MSE: 212.322525\n",
      "Batch 121/600 [20.2%] - Loss: 0.017733, MSE: 201.631348\n",
      "Batch 181/600 [30.2%] - Loss: 0.017054, MSE: 183.281784\n",
      "Batch 241/600 [40.2%] - Loss: 0.018859, MSE: 204.139664\n",
      "Batch 301/600 [50.2%] - Loss: 0.017956, MSE: 187.218216\n",
      "Batch 361/600 [60.2%] - Loss: 0.019349, MSE: 219.204178\n",
      "Batch 421/600 [70.2%] - Loss: 0.018840, MSE: 213.219360\n",
      "Batch 481/600 [80.2%] - Loss: 0.018790, MSE: 220.929611\n",
      "Batch 541/600 [90.2%] - Loss: 0.018670, MSE: 217.882584\n",
      "Batch 600/600 [100.0%] - Loss: 0.019647, MSE: 206.329834\n",
      "  ✗ No improvement for 4/20 epochs. Best: 0.004456\n",
      "\n",
      "Epoch 34/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.022255, MSE: 238.389603\n",
      "Batch 61/600 [10.2%] - Loss: 0.019958, MSE: 211.183884\n",
      "Batch 121/600 [20.2%] - Loss: 0.019869, MSE: 261.160675\n",
      "Batch 181/600 [30.2%] - Loss: 0.019445, MSE: 228.556473\n",
      "Batch 241/600 [40.2%] - Loss: 0.020005, MSE: 210.770325\n",
      "Batch 301/600 [50.2%] - Loss: 0.018654, MSE: 244.240662\n",
      "Batch 361/600 [60.2%] - Loss: 0.020735, MSE: 203.120697\n",
      "Batch 421/600 [70.2%] - Loss: 0.019266, MSE: 237.290298\n",
      "Batch 481/600 [80.2%] - Loss: 0.017615, MSE: 208.474945\n",
      "Batch 541/600 [90.2%] - Loss: 0.020823, MSE: 248.816650\n",
      "Batch 600/600 [100.0%] - Loss: 0.020085, MSE: 220.784531\n",
      "  ✗ No improvement for 5/20 epochs. Best: 0.004456\n",
      "  → Reducing learning rate: 0.000100 → 0.000100\n",
      "\n",
      "Epoch 35/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.019577, MSE: 218.706375\n",
      "Batch 61/600 [10.2%] - Loss: 0.016580, MSE: 217.086456\n",
      "Batch 121/600 [20.2%] - Loss: 0.020886, MSE: 234.445541\n",
      "Batch 181/600 [30.2%] - Loss: 0.019508, MSE: 245.850113\n",
      "Batch 241/600 [40.2%] - Loss: 0.017965, MSE: 196.221588\n",
      "Batch 301/600 [50.2%] - Loss: 0.020378, MSE: 229.795029\n",
      "Batch 361/600 [60.2%] - Loss: 0.021998, MSE: 188.031708\n",
      "Batch 421/600 [70.2%] - Loss: 0.017342, MSE: 199.764359\n",
      "Batch 481/600 [80.2%] - Loss: 0.018376, MSE: 199.101379\n",
      "Batch 541/600 [90.2%] - Loss: 0.019323, MSE: 224.296890\n",
      "Batch 600/600 [100.0%] - Loss: 0.019479, MSE: 234.865936\n",
      "  ✗ No improvement for 6/20 epochs. Best: 0.004456\n",
      "\n",
      "Epoch 36/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.018015, MSE: 194.245346\n",
      "Batch 61/600 [10.2%] - Loss: 0.020993, MSE: 211.034225\n",
      "Batch 121/600 [20.2%] - Loss: 0.019045, MSE: 239.227417\n",
      "Batch 181/600 [30.2%] - Loss: 0.019108, MSE: 252.222137\n",
      "Batch 241/600 [40.2%] - Loss: 0.019452, MSE: 222.174683\n",
      "Batch 301/600 [50.2%] - Loss: 0.017946, MSE: 232.239120\n",
      "Batch 361/600 [60.2%] - Loss: 0.018436, MSE: 208.262131\n",
      "Batch 421/600 [70.2%] - Loss: 0.021300, MSE: 229.867050\n",
      "Batch 481/600 [80.2%] - Loss: 0.017535, MSE: 210.217758\n",
      "Batch 541/600 [90.2%] - Loss: 0.018046, MSE: 216.704788\n",
      "Batch 600/600 [100.0%] - Loss: 0.021687, MSE: 255.781021\n",
      "Epoch 36/100 - 0:00:11 - ETA: 0:12:24\n",
      "  Train Loss: 0.019646, Train MSE: 222.790452\n",
      "  Val Loss: 0.004552, Val MSE: 16.649321, Val RMSLE: 0.067467\n",
      "  Learning rate: 0.000100\n",
      "  Validation loss change: ↑ 1.65%\n",
      "  ✗ No improvement for 7/20 epochs. Best: 0.004456\n",
      "\n",
      "Epoch 37/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.018965, MSE: 236.329254\n",
      "Batch 61/600 [10.2%] - Loss: 0.022604, MSE: 249.310608\n",
      "Batch 121/600 [20.2%] - Loss: 0.019111, MSE: 201.329254\n",
      "Batch 181/600 [30.2%] - Loss: 0.019550, MSE: 240.037918\n",
      "Batch 241/600 [40.2%] - Loss: 0.019272, MSE: 217.903870\n",
      "Batch 301/600 [50.2%] - Loss: 0.019620, MSE: 236.867081\n",
      "Batch 361/600 [60.2%] - Loss: 0.018287, MSE: 199.912170\n",
      "Batch 421/600 [70.2%] - Loss: 0.019832, MSE: 225.959610\n",
      "Batch 481/600 [80.2%] - Loss: 0.019444, MSE: 227.575027\n",
      "Batch 541/600 [90.2%] - Loss: 0.019928, MSE: 213.915909\n",
      "Batch 600/600 [100.0%] - Loss: 0.019160, MSE: 243.082413\n",
      "  ✗ No improvement for 8/20 epochs. Best: 0.004456\n",
      "\n",
      "Epoch 38/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.019763, MSE: 222.806198\n",
      "Batch 61/600 [10.2%] - Loss: 0.017956, MSE: 236.589981\n",
      "Batch 121/600 [20.2%] - Loss: 0.024267, MSE: 242.955093\n",
      "Batch 181/600 [30.2%] - Loss: 0.019705, MSE: 224.352539\n",
      "Batch 241/600 [40.2%] - Loss: 0.018999, MSE: 206.787933\n",
      "Batch 301/600 [50.2%] - Loss: 0.019482, MSE: 207.903870\n",
      "Batch 361/600 [60.2%] - Loss: 0.019006, MSE: 219.097443\n",
      "Batch 421/600 [70.2%] - Loss: 0.021783, MSE: 234.274841\n",
      "Batch 481/600 [80.2%] - Loss: 0.021185, MSE: 233.933441\n",
      "Batch 541/600 [90.2%] - Loss: 0.018206, MSE: 211.829788\n",
      "Batch 600/600 [100.0%] - Loss: 0.021938, MSE: 225.591980\n",
      "  ✗ No improvement for 9/20 epochs. Best: 0.004456\n",
      "\n",
      "Epoch 39/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.019009, MSE: 228.014359\n",
      "Batch 61/600 [10.2%] - Loss: 0.020349, MSE: 229.071884\n",
      "Batch 121/600 [20.2%] - Loss: 0.017740, MSE: 239.233047\n",
      "Batch 181/600 [30.2%] - Loss: 0.019717, MSE: 213.380722\n",
      "Batch 241/600 [40.2%] - Loss: 0.018779, MSE: 224.452316\n",
      "Batch 301/600 [50.2%] - Loss: 0.019207, MSE: 236.127090\n",
      "Batch 361/600 [60.2%] - Loss: 0.018649, MSE: 212.625961\n",
      "Batch 421/600 [70.2%] - Loss: 0.020284, MSE: 218.075790\n",
      "Batch 481/600 [80.2%] - Loss: 0.017265, MSE: 239.940018\n",
      "Batch 541/600 [90.2%] - Loss: 0.020243, MSE: 270.490387\n",
      "Batch 600/600 [100.0%] - Loss: 0.019197, MSE: 183.334534\n",
      "  ✗ No improvement for 10/20 epochs. Best: 0.004456\n",
      "  → Reducing learning rate: 0.000100 → 0.000100\n",
      "\n",
      "Epoch 40/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.019994, MSE: 249.333588\n",
      "Batch 61/600 [10.2%] - Loss: 0.019677, MSE: 201.322708\n",
      "Batch 121/600 [20.2%] - Loss: 0.024989, MSE: 223.848175\n",
      "Batch 181/600 [30.2%] - Loss: 0.018622, MSE: 206.580978\n",
      "Batch 241/600 [40.2%] - Loss: 0.020299, MSE: 217.709412\n",
      "Batch 301/600 [50.2%] - Loss: 0.019100, MSE: 216.194901\n",
      "Batch 361/600 [60.2%] - Loss: 0.022420, MSE: 210.824814\n",
      "Batch 421/600 [70.2%] - Loss: 0.028749, MSE: 247.956772\n",
      "Batch 481/600 [80.2%] - Loss: 0.019025, MSE: 255.833878\n",
      "Batch 541/600 [90.2%] - Loss: 0.019496, MSE: 232.889923\n",
      "Batch 600/600 [100.0%] - Loss: 0.018248, MSE: 218.155365\n",
      "  ✗ No improvement for 11/20 epochs. Best: 0.004456\n",
      "\n",
      "Epoch 41/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.019426, MSE: 192.527603\n",
      "Batch 61/600 [10.2%] - Loss: 0.018604, MSE: 185.809128\n",
      "Batch 121/600 [20.2%] - Loss: 0.020015, MSE: 233.542984\n",
      "Batch 181/600 [30.2%] - Loss: 0.018867, MSE: 248.600174\n",
      "Batch 241/600 [40.2%] - Loss: 0.018865, MSE: 230.174469\n",
      "Batch 301/600 [50.2%] - Loss: 0.019633, MSE: 230.922852\n",
      "Batch 361/600 [60.2%] - Loss: 0.019154, MSE: 219.057281\n",
      "Batch 421/600 [70.2%] - Loss: 0.018599, MSE: 232.877075\n",
      "Batch 481/600 [80.2%] - Loss: 0.018080, MSE: 209.060349\n",
      "Batch 541/600 [90.2%] - Loss: 0.019554, MSE: 217.937088\n",
      "Batch 600/600 [100.0%] - Loss: 0.018005, MSE: 265.451508\n",
      "Epoch 41/100 - 0:00:12 - ETA: 0:11:28\n",
      "  Train Loss: 0.019621, Train MSE: 220.938394\n",
      "  Val Loss: 0.004537, Val MSE: 16.539881, Val RMSLE: 0.067354\n",
      "  Learning rate: 0.000100\n",
      "  Validation loss change: ↑ 0.63%\n",
      "  ✗ No improvement for 12/20 epochs. Best: 0.004456\n",
      "\n",
      "Epoch 42/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.019437, MSE: 216.986832\n",
      "Batch 61/600 [10.2%] - Loss: 0.018691, MSE: 229.604507\n",
      "Batch 121/600 [20.2%] - Loss: 0.021457, MSE: 260.127045\n",
      "Batch 181/600 [30.2%] - Loss: 0.020639, MSE: 237.086761\n",
      "Batch 241/600 [40.2%] - Loss: 0.028157, MSE: 219.902390\n",
      "Batch 301/600 [50.2%] - Loss: 0.021238, MSE: 262.859955\n",
      "Batch 361/600 [60.2%] - Loss: 0.017587, MSE: 223.120361\n",
      "Batch 421/600 [70.2%] - Loss: 0.018256, MSE: 193.162949\n",
      "Batch 481/600 [80.2%] - Loss: 0.018564, MSE: 194.065201\n",
      "Batch 541/600 [90.2%] - Loss: 0.019582, MSE: 239.705673\n",
      "Batch 600/600 [100.0%] - Loss: 0.018851, MSE: 228.155762\n",
      "  ✗ No improvement for 13/20 epochs. Best: 0.004456\n",
      "\n",
      "Epoch 43/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.017840, MSE: 227.345169\n",
      "Batch 61/600 [10.2%] - Loss: 0.019371, MSE: 224.547195\n",
      "Batch 121/600 [20.2%] - Loss: 0.017891, MSE: 200.774353\n",
      "Batch 181/600 [30.2%] - Loss: 0.018728, MSE: 202.929413\n",
      "Batch 241/600 [40.2%] - Loss: 0.021963, MSE: 245.889267\n",
      "Batch 301/600 [50.2%] - Loss: 0.019570, MSE: 226.602921\n",
      "Batch 361/600 [60.2%] - Loss: 0.019279, MSE: 215.216309\n",
      "Batch 421/600 [70.2%] - Loss: 0.019891, MSE: 207.391373\n",
      "Batch 481/600 [80.2%] - Loss: 0.021152, MSE: 240.319794\n",
      "Batch 541/600 [90.2%] - Loss: 0.021473, MSE: 221.131287\n",
      "Batch 600/600 [100.0%] - Loss: 0.017857, MSE: 219.679825\n",
      "  ✗ No improvement for 14/20 epochs. Best: 0.004456\n",
      "\n",
      "Epoch 44/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.018216, MSE: 214.969452\n",
      "Batch 61/600 [10.2%] - Loss: 0.022042, MSE: 237.082184\n",
      "Batch 121/600 [20.2%] - Loss: 0.017012, MSE: 227.384186\n",
      "Batch 181/600 [30.2%] - Loss: 0.019438, MSE: 193.711502\n",
      "Batch 241/600 [40.2%] - Loss: 0.018718, MSE: 212.817017\n",
      "Batch 301/600 [50.2%] - Loss: 0.023152, MSE: 225.311890\n",
      "Batch 361/600 [60.2%] - Loss: 0.018096, MSE: 198.265671\n",
      "Batch 421/600 [70.2%] - Loss: 0.019344, MSE: 251.769897\n",
      "Batch 481/600 [80.2%] - Loss: 0.020580, MSE: 246.076385\n",
      "Batch 541/600 [90.2%] - Loss: 0.018099, MSE: 194.687302\n",
      "Batch 600/600 [100.0%] - Loss: 0.019387, MSE: 238.604294\n",
      "  ✗ No improvement for 15/20 epochs. Best: 0.004456\n",
      "  → Reducing learning rate: 0.000100 → 0.000100\n",
      "\n",
      "Epoch 45/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.020244, MSE: 227.552887\n",
      "Batch 61/600 [10.2%] - Loss: 0.018309, MSE: 191.775253\n",
      "Batch 121/600 [20.2%] - Loss: 0.020070, MSE: 238.887115\n",
      "Batch 181/600 [30.2%] - Loss: 0.019777, MSE: 225.245834\n",
      "Batch 241/600 [40.2%] - Loss: 0.020512, MSE: 225.951828\n",
      "Batch 301/600 [50.2%] - Loss: 0.021767, MSE: 228.999954\n",
      "Batch 361/600 [60.2%] - Loss: 0.019200, MSE: 209.871429\n",
      "Batch 421/600 [70.2%] - Loss: 0.018503, MSE: 210.763885\n",
      "Batch 481/600 [80.2%] - Loss: 0.017532, MSE: 216.149017\n",
      "Batch 541/600 [90.2%] - Loss: 0.019253, MSE: 230.091919\n",
      "Batch 600/600 [100.0%] - Loss: 0.021239, MSE: 259.308685\n",
      "  ✗ No improvement for 16/20 epochs. Best: 0.004456\n",
      "\n",
      "Epoch 46/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.018819, MSE: 216.411331\n",
      "Batch 61/600 [10.2%] - Loss: 0.020915, MSE: 225.915009\n",
      "Batch 121/600 [20.2%] - Loss: 0.021241, MSE: 230.221268\n",
      "Batch 181/600 [30.2%] - Loss: 0.018872, MSE: 242.645660\n",
      "Batch 241/600 [40.2%] - Loss: 0.018034, MSE: 209.904068\n",
      "Batch 301/600 [50.2%] - Loss: 0.021754, MSE: 238.663986\n",
      "Batch 361/600 [60.2%] - Loss: 0.017671, MSE: 212.230408\n",
      "Batch 421/600 [70.2%] - Loss: 0.018867, MSE: 229.727005\n",
      "Batch 481/600 [80.2%] - Loss: 0.020533, MSE: 251.359406\n",
      "Batch 541/600 [90.2%] - Loss: 0.016107, MSE: 190.207962\n",
      "Batch 600/600 [100.0%] - Loss: 0.021226, MSE: 239.616531\n",
      "Epoch 46/100 - 0:00:11 - ETA: 0:10:31\n",
      "  Train Loss: 0.019584, Train MSE: 221.210812\n",
      "  Val Loss: 0.004550, Val MSE: 16.638412, Val RMSLE: 0.067452\n",
      "  Learning rate: 0.000100\n",
      "  Validation loss change: ↑ 1.57%\n",
      "  ✗ No improvement for 17/20 epochs. Best: 0.004456\n",
      "\n",
      "Epoch 47/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.017539, MSE: 189.991669\n",
      "Batch 61/600 [10.2%] - Loss: 0.019921, MSE: 216.978973\n",
      "Batch 121/600 [20.2%] - Loss: 0.019064, MSE: 198.500320\n",
      "Batch 181/600 [30.2%] - Loss: 0.019153, MSE: 207.006668\n",
      "Batch 241/600 [40.2%] - Loss: 0.020129, MSE: 233.781281\n",
      "Batch 301/600 [50.2%] - Loss: 0.018020, MSE: 211.065109\n",
      "Batch 361/600 [60.2%] - Loss: 0.019462, MSE: 241.889404\n",
      "Batch 421/600 [70.2%] - Loss: 0.020129, MSE: 247.760574\n",
      "Batch 481/600 [80.2%] - Loss: 0.018448, MSE: 210.065842\n",
      "Batch 541/600 [90.2%] - Loss: 0.019107, MSE: 252.393433\n",
      "Batch 600/600 [100.0%] - Loss: 0.019333, MSE: 216.196182\n",
      "  ✓ New best validation loss: 0.004452 (improved by 0.09%)\n",
      "\n",
      "Epoch 48/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.018098, MSE: 198.391846\n",
      "Batch 61/600 [10.2%] - Loss: 0.019695, MSE: 235.590195\n",
      "Batch 121/600 [20.2%] - Loss: 0.019039, MSE: 204.303482\n",
      "Batch 181/600 [30.2%] - Loss: 0.025474, MSE: 201.795853\n",
      "Batch 241/600 [40.2%] - Loss: 0.020588, MSE: 225.087357\n",
      "Batch 301/600 [50.2%] - Loss: 0.021012, MSE: 245.725723\n",
      "Batch 361/600 [60.2%] - Loss: 0.028012, MSE: 211.321869\n",
      "Batch 421/600 [70.2%] - Loss: 0.019579, MSE: 235.409454\n",
      "Batch 481/600 [80.2%] - Loss: 0.020026, MSE: 206.528763\n",
      "Batch 541/600 [90.2%] - Loss: 0.019725, MSE: 202.359741\n",
      "Batch 600/600 [100.0%] - Loss: 0.018907, MSE: 221.496597\n",
      "  ✗ No improvement for 1/20 epochs. Best: 0.004452\n",
      "\n",
      "Epoch 49/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.019215, MSE: 227.521255\n",
      "Batch 61/600 [10.2%] - Loss: 0.019823, MSE: 237.754852\n",
      "Batch 121/600 [20.2%] - Loss: 0.018944, MSE: 227.804108\n",
      "Batch 181/600 [30.2%] - Loss: 0.019905, MSE: 221.586197\n",
      "Batch 241/600 [40.2%] - Loss: 0.017511, MSE: 204.181152\n",
      "Batch 301/600 [50.2%] - Loss: 0.019237, MSE: 198.844040\n",
      "Batch 361/600 [60.2%] - Loss: 0.018857, MSE: 224.165710\n",
      "Batch 421/600 [70.2%] - Loss: 0.022961, MSE: 232.119598\n",
      "Batch 481/600 [80.2%] - Loss: 0.019174, MSE: 221.656372\n",
      "Batch 541/600 [90.2%] - Loss: 0.017983, MSE: 203.051392\n",
      "Batch 600/600 [100.0%] - Loss: 0.021063, MSE: 247.938293\n",
      "  ✗ No improvement for 2/20 epochs. Best: 0.004452\n",
      "\n",
      "Epoch 50/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.019177, MSE: 223.491013\n",
      "Batch 61/600 [10.2%] - Loss: 0.019330, MSE: 226.076447\n",
      "Batch 121/600 [20.2%] - Loss: 0.019817, MSE: 203.451706\n",
      "Batch 181/600 [30.2%] - Loss: 0.022506, MSE: 243.125031\n",
      "Batch 241/600 [40.2%] - Loss: 0.018615, MSE: 201.848541\n",
      "Batch 301/600 [50.2%] - Loss: 0.018835, MSE: 224.780807\n",
      "Batch 361/600 [60.2%] - Loss: 0.019472, MSE: 222.805069\n",
      "Batch 421/600 [70.2%] - Loss: 0.019690, MSE: 232.717896\n",
      "Batch 481/600 [80.2%] - Loss: 0.019179, MSE: 215.704636\n",
      "Batch 541/600 [90.2%] - Loss: 0.019855, MSE: 231.447922\n",
      "Batch 600/600 [100.0%] - Loss: 0.022693, MSE: 213.214600\n",
      "  ✗ No improvement for 3/20 epochs. Best: 0.004452\n",
      "\n",
      "Epoch 51/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.020155, MSE: 207.755692\n",
      "Batch 61/600 [10.2%] - Loss: 0.019031, MSE: 224.244461\n",
      "Batch 121/600 [20.2%] - Loss: 0.019082, MSE: 233.976257\n",
      "Batch 181/600 [30.2%] - Loss: 0.018906, MSE: 226.812851\n",
      "Batch 241/600 [40.2%] - Loss: 0.020273, MSE: 220.188828\n",
      "Batch 301/600 [50.2%] - Loss: 0.019465, MSE: 228.025589\n",
      "Batch 361/600 [60.2%] - Loss: 0.020037, MSE: 216.900009\n",
      "Batch 421/600 [70.2%] - Loss: 0.020567, MSE: 207.625443\n",
      "Batch 481/600 [80.2%] - Loss: 0.018778, MSE: 227.227005\n",
      "Batch 541/600 [90.2%] - Loss: 0.017525, MSE: 193.717377\n",
      "Batch 600/600 [100.0%] - Loss: 0.023137, MSE: 235.128860\n",
      "Epoch 51/100 - 0:00:11 - ETA: 0:09:34\n",
      "  Train Loss: 0.019634, Train MSE: 221.938648\n",
      "  Val Loss: 0.004522, Val MSE: 16.362513, Val RMSLE: 0.067246\n",
      "  Learning rate: 0.000100\n",
      "  Validation loss change: ↓ 1.12%\n",
      "  ✗ No improvement for 4/20 epochs. Best: 0.004452\n",
      "\n",
      "Epoch 52/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.018909, MSE: 225.545486\n",
      "Batch 61/600 [10.2%] - Loss: 0.025402, MSE: 232.572418\n",
      "Batch 121/600 [20.2%] - Loss: 0.018984, MSE: 207.776535\n",
      "Batch 181/600 [30.2%] - Loss: 0.017825, MSE: 255.216415\n",
      "Batch 241/600 [40.2%] - Loss: 0.018569, MSE: 207.429611\n",
      "Batch 301/600 [50.2%] - Loss: 0.021312, MSE: 242.981918\n",
      "Batch 361/600 [60.2%] - Loss: 0.018771, MSE: 217.219193\n",
      "Batch 421/600 [70.2%] - Loss: 0.018778, MSE: 218.708710\n",
      "Batch 481/600 [80.2%] - Loss: 0.018622, MSE: 220.638351\n",
      "Batch 541/600 [90.2%] - Loss: 0.018510, MSE: 250.636139\n",
      "Batch 600/600 [100.0%] - Loss: 0.018175, MSE: 221.827621\n",
      "  ✗ No improvement for 5/20 epochs. Best: 0.004452\n",
      "  → Reducing learning rate: 0.000100 → 0.000100\n",
      "\n",
      "Epoch 53/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.022280, MSE: 275.721863\n",
      "Batch 61/600 [10.2%] - Loss: 0.020237, MSE: 222.287827\n",
      "Batch 121/600 [20.2%] - Loss: 0.019896, MSE: 242.108627\n",
      "Batch 181/600 [30.2%] - Loss: 0.024966, MSE: 235.802170\n",
      "Batch 241/600 [40.2%] - Loss: 0.020373, MSE: 223.438904\n",
      "Batch 301/600 [50.2%] - Loss: 0.018831, MSE: 239.778076\n",
      "Batch 361/600 [60.2%] - Loss: 0.018795, MSE: 210.297760\n",
      "Batch 421/600 [70.2%] - Loss: 0.020322, MSE: 214.395035\n",
      "Batch 481/600 [80.2%] - Loss: 0.021545, MSE: 208.660538\n",
      "Batch 541/600 [90.2%] - Loss: 0.018246, MSE: 213.474533\n",
      "Batch 600/600 [100.0%] - Loss: 0.019033, MSE: 214.149567\n",
      "  ✗ No improvement for 6/20 epochs. Best: 0.004452\n",
      "\n",
      "Epoch 54/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.019325, MSE: 208.982971\n",
      "Batch 61/600 [10.2%] - Loss: 0.018576, MSE: 212.638092\n",
      "Batch 121/600 [20.2%] - Loss: 0.019173, MSE: 237.307007\n",
      "Batch 181/600 [30.2%] - Loss: 0.019748, MSE: 212.533508\n",
      "Batch 241/600 [40.2%] - Loss: 0.019154, MSE: 223.675049\n",
      "Batch 301/600 [50.2%] - Loss: 0.021426, MSE: 257.350983\n",
      "Batch 361/600 [60.2%] - Loss: 0.018639, MSE: 218.380066\n",
      "Batch 421/600 [70.2%] - Loss: 0.020019, MSE: 240.164795\n",
      "Batch 481/600 [80.2%] - Loss: 0.018763, MSE: 192.378891\n",
      "Batch 541/600 [90.2%] - Loss: 0.020717, MSE: 222.992645\n",
      "Batch 600/600 [100.0%] - Loss: 0.018666, MSE: 213.971176\n",
      "  ✗ No improvement for 7/20 epochs. Best: 0.004452\n",
      "\n",
      "Epoch 55/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.021097, MSE: 223.675293\n",
      "Batch 61/600 [10.2%] - Loss: 0.019463, MSE: 223.124603\n",
      "Batch 121/600 [20.2%] - Loss: 0.024217, MSE: 252.096497\n",
      "Batch 181/600 [30.2%] - Loss: 0.021241, MSE: 278.500458\n",
      "Batch 241/600 [40.2%] - Loss: 0.019023, MSE: 194.197601\n",
      "Batch 301/600 [50.2%] - Loss: 0.017770, MSE: 205.567230\n",
      "Batch 361/600 [60.2%] - Loss: 0.019839, MSE: 219.554520\n",
      "Batch 421/600 [70.2%] - Loss: 0.017992, MSE: 204.141663\n",
      "Batch 481/600 [80.2%] - Loss: 0.019410, MSE: 245.396210\n",
      "Batch 541/600 [90.2%] - Loss: 0.020272, MSE: 231.392349\n",
      "Batch 600/600 [100.0%] - Loss: 0.019581, MSE: 216.892319\n",
      "  ✗ No improvement for 8/20 epochs. Best: 0.004452\n",
      "\n",
      "Epoch 56/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.018651, MSE: 233.015305\n",
      "Batch 61/600 [10.2%] - Loss: 0.017730, MSE: 201.049545\n",
      "Batch 121/600 [20.2%] - Loss: 0.019456, MSE: 229.784866\n",
      "Batch 181/600 [30.2%] - Loss: 0.016957, MSE: 210.817230\n",
      "Batch 241/600 [40.2%] - Loss: 0.020151, MSE: 220.686508\n",
      "Batch 301/600 [50.2%] - Loss: 0.017410, MSE: 193.369537\n",
      "Batch 361/600 [60.2%] - Loss: 0.019921, MSE: 258.007141\n",
      "Batch 421/600 [70.2%] - Loss: 0.018497, MSE: 215.144974\n",
      "Batch 481/600 [80.2%] - Loss: 0.020773, MSE: 260.509888\n",
      "Batch 541/600 [90.2%] - Loss: 0.018144, MSE: 217.449234\n",
      "Batch 600/600 [100.0%] - Loss: 0.020234, MSE: 212.440628\n",
      "Epoch 56/100 - 0:00:11 - ETA: 0:08:36\n",
      "  Train Loss: 0.019545, Train MSE: 221.655253\n",
      "  Val Loss: 0.004577, Val MSE: 16.336327, Val RMSLE: 0.067652\n",
      "  Learning rate: 0.000100\n",
      "  Validation loss change: ↑ 2.09%\n",
      "  ✗ No improvement for 9/20 epochs. Best: 0.004452\n",
      "\n",
      "Epoch 57/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.020682, MSE: 231.947159\n",
      "Batch 61/600 [10.2%] - Loss: 0.019338, MSE: 218.897446\n",
      "Batch 121/600 [20.2%] - Loss: 0.026977, MSE: 240.810349\n",
      "Batch 181/600 [30.2%] - Loss: 0.019158, MSE: 246.520874\n",
      "Batch 241/600 [40.2%] - Loss: 0.019998, MSE: 218.812851\n",
      "Batch 301/600 [50.2%] - Loss: 0.018361, MSE: 210.744095\n",
      "Batch 361/600 [60.2%] - Loss: 0.016446, MSE: 191.097885\n",
      "Batch 421/600 [70.2%] - Loss: 0.019556, MSE: 213.238922\n",
      "Batch 481/600 [80.2%] - Loss: 0.018847, MSE: 223.499847\n",
      "Batch 541/600 [90.2%] - Loss: 0.018004, MSE: 197.426987\n",
      "Batch 600/600 [100.0%] - Loss: 0.018600, MSE: 226.541733\n",
      "  ✗ No improvement for 10/20 epochs. Best: 0.004452\n",
      "  → Reducing learning rate: 0.000100 → 0.000100\n",
      "\n",
      "Epoch 58/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.021658, MSE: 209.540359\n",
      "Batch 61/600 [10.2%] - Loss: 0.018949, MSE: 226.420044\n",
      "Batch 121/600 [20.2%] - Loss: 0.019676, MSE: 203.924042\n",
      "Batch 181/600 [30.2%] - Loss: 0.019326, MSE: 200.766022\n",
      "Batch 241/600 [40.2%] - Loss: 0.020517, MSE: 192.289047\n",
      "Batch 301/600 [50.2%] - Loss: 0.020596, MSE: 237.345612\n",
      "Batch 361/600 [60.2%] - Loss: 0.019932, MSE: 254.763947\n",
      "Batch 421/600 [70.2%] - Loss: 0.021790, MSE: 232.003143\n",
      "Batch 481/600 [80.2%] - Loss: 0.019117, MSE: 199.705139\n",
      "Batch 541/600 [90.2%] - Loss: 0.018396, MSE: 208.632019\n",
      "Batch 600/600 [100.0%] - Loss: 0.017683, MSE: 208.761597\n",
      "  ✗ No improvement for 11/20 epochs. Best: 0.004452\n",
      "\n",
      "Epoch 59/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.016511, MSE: 192.021545\n",
      "Batch 61/600 [10.2%] - Loss: 0.019062, MSE: 206.242386\n",
      "Batch 121/600 [20.2%] - Loss: 0.019832, MSE: 268.424530\n",
      "Batch 181/600 [30.2%] - Loss: 0.021714, MSE: 243.567535\n",
      "Batch 241/600 [40.2%] - Loss: 0.019529, MSE: 233.521942\n",
      "Batch 301/600 [50.2%] - Loss: 0.019206, MSE: 202.982941\n",
      "Batch 361/600 [60.2%] - Loss: 0.017647, MSE: 208.445145\n",
      "Batch 421/600 [70.2%] - Loss: 0.015852, MSE: 203.523041\n",
      "Batch 481/600 [80.2%] - Loss: 0.020929, MSE: 212.771576\n",
      "Batch 541/600 [90.2%] - Loss: 0.019813, MSE: 212.258072\n",
      "Batch 600/600 [100.0%] - Loss: 0.020941, MSE: 217.245590\n",
      "  ✗ No improvement for 12/20 epochs. Best: 0.004452\n",
      "\n",
      "Epoch 60/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.018883, MSE: 208.669601\n",
      "Batch 61/600 [10.2%] - Loss: 0.020288, MSE: 218.949722\n",
      "Batch 121/600 [20.2%] - Loss: 0.021446, MSE: 193.462128\n",
      "Batch 181/600 [30.2%] - Loss: 0.017734, MSE: 219.790131\n",
      "Batch 241/600 [40.2%] - Loss: 0.020449, MSE: 220.968048\n",
      "Batch 301/600 [50.2%] - Loss: 0.024544, MSE: 245.789963\n",
      "Batch 361/600 [60.2%] - Loss: 0.018102, MSE: 233.777008\n",
      "Batch 421/600 [70.2%] - Loss: 0.020043, MSE: 238.601837\n",
      "Batch 481/600 [80.2%] - Loss: 0.018913, MSE: 219.953491\n",
      "Batch 541/600 [90.2%] - Loss: 0.021205, MSE: 226.586105\n",
      "Batch 600/600 [100.0%] - Loss: 0.020034, MSE: 244.742233\n",
      "  ✗ No improvement for 13/20 epochs. Best: 0.004452\n",
      "\n",
      "Epoch 61/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.019247, MSE: 236.385040\n",
      "Batch 61/600 [10.2%] - Loss: 0.019736, MSE: 245.942902\n",
      "Batch 121/600 [20.2%] - Loss: 0.019339, MSE: 234.061417\n",
      "Batch 181/600 [30.2%] - Loss: 0.021406, MSE: 249.749222\n",
      "Batch 241/600 [40.2%] - Loss: 0.019001, MSE: 255.546753\n",
      "Batch 301/600 [50.2%] - Loss: 0.020524, MSE: 227.450287\n",
      "Batch 361/600 [60.2%] - Loss: 0.018791, MSE: 215.309296\n",
      "Batch 421/600 [70.2%] - Loss: 0.018180, MSE: 210.661850\n",
      "Batch 481/600 [80.2%] - Loss: 0.020928, MSE: 241.554871\n",
      "Batch 541/600 [90.2%] - Loss: 0.019722, MSE: 247.708847\n",
      "Batch 600/600 [100.0%] - Loss: 0.018330, MSE: 205.815048\n",
      "Epoch 61/100 - 0:00:11 - ETA: 0:07:36\n",
      "  Train Loss: 0.019636, Train MSE: 222.473099\n",
      "  Val Loss: 0.004561, Val MSE: 16.344999, Val RMSLE: 0.067535\n",
      "  Learning rate: 0.000100\n",
      "  Validation loss change: ↑ 1.55%\n",
      "  ✗ No improvement for 14/20 epochs. Best: 0.004452\n",
      "\n",
      "Epoch 62/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.018527, MSE: 214.438339\n",
      "Batch 61/600 [10.2%] - Loss: 0.020578, MSE: 253.538025\n",
      "Batch 121/600 [20.2%] - Loss: 0.020588, MSE: 227.686890\n",
      "Batch 181/600 [30.2%] - Loss: 0.019231, MSE: 250.583328\n",
      "Batch 241/600 [40.2%] - Loss: 0.019473, MSE: 192.326294\n",
      "Batch 301/600 [50.2%] - Loss: 0.019136, MSE: 231.932205\n",
      "Batch 361/600 [60.2%] - Loss: 0.017747, MSE: 223.639633\n",
      "Batch 421/600 [70.2%] - Loss: 0.021222, MSE: 225.285767\n",
      "Batch 481/600 [80.2%] - Loss: 0.018312, MSE: 228.300735\n",
      "Batch 541/600 [90.2%] - Loss: 0.021176, MSE: 216.029602\n",
      "Batch 600/600 [100.0%] - Loss: 0.018094, MSE: 197.786346\n",
      "  ✗ No improvement for 15/20 epochs. Best: 0.004452\n",
      "  → Reducing learning rate: 0.000100 → 0.000100\n",
      "\n",
      "Epoch 63/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.019369, MSE: 205.091827\n",
      "Batch 61/600 [10.2%] - Loss: 0.019725, MSE: 227.284454\n",
      "Batch 121/600 [20.2%] - Loss: 0.020353, MSE: 250.488693\n",
      "Batch 181/600 [30.2%] - Loss: 0.018729, MSE: 218.450607\n",
      "Batch 241/600 [40.2%] - Loss: 0.023776, MSE: 228.149887\n",
      "Batch 301/600 [50.2%] - Loss: 0.018374, MSE: 230.356705\n",
      "Batch 361/600 [60.2%] - Loss: 0.018311, MSE: 218.858597\n",
      "Batch 421/600 [70.2%] - Loss: 0.019337, MSE: 223.592758\n",
      "Batch 481/600 [80.2%] - Loss: 0.018183, MSE: 215.218155\n",
      "Batch 541/600 [90.2%] - Loss: 0.018602, MSE: 194.851730\n",
      "Batch 600/600 [100.0%] - Loss: 0.019914, MSE: 217.975510\n",
      "  ✗ No improvement for 16/20 epochs. Best: 0.004452\n",
      "\n",
      "Epoch 64/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.019019, MSE: 213.566864\n",
      "Batch 61/600 [10.2%] - Loss: 0.018439, MSE: 234.064636\n",
      "Batch 121/600 [20.2%] - Loss: 0.019415, MSE: 230.531921\n",
      "Batch 181/600 [30.2%] - Loss: 0.020588, MSE: 205.822632\n",
      "Batch 241/600 [40.2%] - Loss: 0.018463, MSE: 218.371353\n",
      "Batch 301/600 [50.2%] - Loss: 0.017285, MSE: 202.627640\n",
      "Batch 361/600 [60.2%] - Loss: 0.020240, MSE: 211.271851\n",
      "Batch 421/600 [70.2%] - Loss: 0.018670, MSE: 228.707504\n",
      "Batch 481/600 [80.2%] - Loss: 0.017678, MSE: 204.070129\n",
      "Batch 541/600 [90.2%] - Loss: 0.017954, MSE: 208.191528\n",
      "Batch 600/600 [100.0%] - Loss: 0.018233, MSE: 202.915115\n",
      "  ✗ No improvement for 17/20 epochs. Best: 0.004452\n",
      "\n",
      "Epoch 65/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.019123, MSE: 253.495148\n",
      "Batch 61/600 [10.2%] - Loss: 0.018166, MSE: 241.134140\n",
      "Batch 121/600 [20.2%] - Loss: 0.018868, MSE: 238.164337\n",
      "Batch 181/600 [30.2%] - Loss: 0.022353, MSE: 242.165100\n",
      "Batch 241/600 [40.2%] - Loss: 0.018768, MSE: 205.424133\n",
      "Batch 301/600 [50.2%] - Loss: 0.018605, MSE: 218.067917\n",
      "Batch 361/600 [60.2%] - Loss: 0.020287, MSE: 211.412613\n",
      "Batch 421/600 [70.2%] - Loss: 0.021735, MSE: 246.693420\n",
      "Batch 481/600 [80.2%] - Loss: 0.017243, MSE: 227.521515\n",
      "Batch 541/600 [90.2%] - Loss: 0.018900, MSE: 215.749542\n",
      "Batch 600/600 [100.0%] - Loss: 0.017863, MSE: 246.332077\n",
      "  ✗ No improvement for 18/20 epochs. Best: 0.004452\n",
      "\n",
      "Epoch 66/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.023504, MSE: 227.994202\n",
      "Batch 61/600 [10.2%] - Loss: 0.019340, MSE: 236.177231\n",
      "Batch 121/600 [20.2%] - Loss: 0.018681, MSE: 213.524139\n",
      "Batch 181/600 [30.2%] - Loss: 0.018229, MSE: 216.010391\n",
      "Batch 241/600 [40.2%] - Loss: 0.019652, MSE: 232.929535\n",
      "Batch 301/600 [50.2%] - Loss: 0.019356, MSE: 220.040955\n",
      "Batch 361/600 [60.2%] - Loss: 0.019688, MSE: 229.210892\n",
      "Batch 421/600 [70.2%] - Loss: 0.019283, MSE: 240.867966\n",
      "Batch 481/600 [80.2%] - Loss: 0.020325, MSE: 245.565613\n",
      "Batch 541/600 [90.2%] - Loss: 0.019553, MSE: 236.511200\n",
      "Batch 600/600 [100.0%] - Loss: 0.019506, MSE: 214.612564\n",
      "Epoch 66/100 - 0:00:11 - ETA: 0:06:38\n",
      "  Train Loss: 0.019532, Train MSE: 221.085119\n",
      "  Val Loss: 0.004520, Val MSE: 16.347582, Val RMSLE: 0.067231\n",
      "  Learning rate: 0.000100\n",
      "  Validation loss change: ↑ 0.14%\n",
      "  ✗ No improvement for 19/20 epochs. Best: 0.004452\n",
      "\n",
      "Epoch 67/100:\n",
      "==================================================\n",
      "Batch 1/600 [0.2%] - Loss: 0.020483, MSE: 237.711487\n",
      "Batch 61/600 [10.2%] - Loss: 0.020045, MSE: 253.695892\n",
      "Batch 121/600 [20.2%] - Loss: 0.019105, MSE: 220.963974\n",
      "Batch 181/600 [30.2%] - Loss: 0.019914, MSE: 258.731323\n",
      "Batch 241/600 [40.2%] - Loss: 0.017953, MSE: 206.088715\n",
      "Batch 301/600 [50.2%] - Loss: 0.019807, MSE: 223.622101\n",
      "Batch 361/600 [60.2%] - Loss: 0.018238, MSE: 214.100739\n",
      "Batch 421/600 [70.2%] - Loss: 0.017123, MSE: 209.142670\n",
      "Batch 481/600 [80.2%] - Loss: 0.021707, MSE: 211.831253\n",
      "Batch 541/600 [90.2%] - Loss: 0.018646, MSE: 210.008896\n",
      "Batch 600/600 [100.0%] - Loss: 0.019432, MSE: 214.736465\n",
      "  ✗ No improvement for 20/20 epochs. Best: 0.004452\n",
      "\n",
      "==================================================\n",
      "Early stopping at epoch 67\n",
      "Best validation loss: 0.004452\n",
      "==================================================\n",
      "\n",
      "================================================================================\n",
      "Training completed in 0:13:04\n",
      "Best validation loss: 0.004452\n",
      "Total epochs: 67/100\n",
      "Average epoch time: 0:00:11\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T00:53:38.615278Z",
     "start_time": "2025-05-09T00:53:38.470910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['loss'], label='Training Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['mse'], label='Training MSE')\n",
    "plt.plot(history['val_mse'], label='Validation MSE')\n",
    "plt.title('Mean Squared Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "ad21007a857d5100",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T00:53:52.962768Z",
     "start_time": "2025-05-09T00:53:52.915241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Make predictions on the validation set\n",
    "basic_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_basic = basic_model(X_val_tensor).cpu().numpy().flatten()\n",
    "\n",
    "# Ensure predictions are positive (required for log calculation)\n",
    "y_pred_basic = np.maximum(y_pred_basic, 0)\n",
    "\n",
    "# Calculate RMSLE (Root Mean Squared Logarithmic Error)\n",
    "def rmsle(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "# Calculate metrics\n",
    "rmsle_score = rmsle(y_val, y_pred_basic)\n",
    "rmse_score = np.sqrt(mean_squared_error(y_val, y_pred_basic))\n",
    "r2 = r2_score(y_val, y_pred_basic)\n",
    "\n",
    "print(f\"Basic Neural Network Model Performance:\")\n",
    "print(f\"RMSLE: {rmsle_score:.4f}\")\n",
    "print(f\"RMSE: {rmse_score:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n"
   ],
   "id": "214fb93cf19ebf61",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Neural Network Model Performance:\n",
      "RMSLE: 0.0677\n",
      "RMSE: 4.0256\n",
      "R² Score: 0.9958\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Hyperparameter Tuning\n",
   "id": "c57d55d540afce88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T00:54:30.012351Z",
     "start_time": "2025-05-09T00:54:30.008568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a function to evaluate a model with given hyperparameters\n",
    "def evaluate_model(hidden_layers, activation, learning_rate, dropout_rate, l1_reg, l2_reg, batch_size):\n",
    "    # Create the model and optimizer\n",
    "    model, optimizer = create_model(\n",
    "        input_dim=input_dim,\n",
    "        hidden_layers=hidden_layers,\n",
    "        activation=activation,\n",
    "        learning_rate=learning_rate,\n",
    "        dropout_rate=dropout_rate,\n",
    "        l1_reg=l1_reg,\n",
    "        l2_reg=l2_reg\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model, history = train_model(\n",
    "        model, optimizer,\n",
    "        X_train_tensor, y_train_tensor,\n",
    "        X_val_tensor, y_val_tensor,\n",
    "        epochs=100,\n",
    "        batch_size=batch_size,\n",
    "        patience=20,\n",
    "        factor=0.2,\n",
    "        min_lr=0.0001,\n",
    "        verbose=1  # Set to 2 for more detailed output\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_val_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    y_pred = np.maximum(y_pred, 0)  # Ensure predictions are positive\n",
    "    rmsle_val = rmsle(y_val, y_pred)\n",
    "\n",
    "    return model, rmsle_val, history\n"
   ],
   "id": "bd71857105a70849",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T01:03:33.559193Z",
     "start_time": "2025-05-09T00:55:32.408263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define hyperparameter combinations to try\n",
    "hyperparameter_combinations = [\n",
    "    # Hidden layers, activation, learning rate, dropout rate, l1 reg, l2 reg, batch size\n",
    "    [[128, 64], 'relu', 0.001, 0.2, 0.0, 0.0, 1500],\n",
    "    [[256, 128, 64], 'relu', 0.001, 0.3, 0.0, 0.001, 1500],\n",
    "    [[128, 64, 32], 'elu', 0.0005, 0.2, 0.0, 0.001, 1500],\n",
    "    [[64, 32], 'relu', 0.001, 0.1, 0.0001, 0.0, 1500],\n",
    "    [[128, 64, 32, 16], 'relu', 0.001, 0.2, 0.0, 0.001, 1500]\n",
    "]\n",
    "\n",
    "# Evaluate each combination\n",
    "results = []\n",
    "for i, params in enumerate(hyperparameter_combinations):\n",
    "    print(f\"Training model {i+1}/{len(hyperparameter_combinations)}...\")\n",
    "    model, rmsle_val, history = evaluate_model(*params)\n",
    "    results.append({\n",
    "        'model': model,\n",
    "        'params': params,\n",
    "        'rmsle': rmsle_val,\n",
    "        'history': history\n",
    "    })\n",
    "    print(f\"RMSLE: {rmsle_val:.4f}\")\n",
    "\n",
    "# Find the best model\n",
    "best_result = min(results, key=lambda x: x['rmsle'])\n",
    "print(\"\\nBest model parameters:\")\n",
    "print(f\"Hidden layers: {best_result['params'][0]}\")\n",
    "print(f\"Activation: {best_result['params'][1]}\")\n",
    "print(f\"Learning rate: {best_result['params'][2]}\")\n",
    "print(f\"Dropout rate: {best_result['params'][3]}\")\n",
    "print(f\"L1 regularization: {best_result['params'][4]}\")\n",
    "print(f\"L2 regularization: {best_result['params'][5]}\")\n",
    "print(f\"Batch size: {best_result['params'][6]}\")\n",
    "print(f\"RMSLE: {best_result['rmsle']:.4f}\")\n"
   ],
   "id": "1679956ecc77ae78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1/5...\n",
      "\n",
      "================================================================================\n",
      "Starting training with 100 epochs, batch size 1500\n",
      "Device: mps, Model parameters: 9729\n",
      "Training samples: 600000, Validation samples: 150000\n",
      "Learning rate: 0.001, Patience: 20\n",
      "================================================================================\n",
      "\n",
      "Epoch 1/100 - 0:00:12 - ETA: 0:19:50\n",
      "  Train Loss: 2.246496, Train MSE: 4969.663527\n",
      "  Val Loss: 0.150580, Val MSE: 530.572998, Val RMSLE: 0.388047\n",
      "  Learning rate: 0.001000\n",
      "  ✓ New best validation loss: 0.150580 (improved by nan%)\n",
      "  ✓ New best validation loss: 0.054775 (improved by 63.62%)\n",
      "  ✓ New best validation loss: 0.033106 (improved by 39.56%)\n",
      "  ✓ New best validation loss: 0.023514 (improved by 28.97%)\n",
      "  ✓ New best validation loss: 0.018168 (improved by 22.73%)\n",
      "Epoch 6/100 - 0:00:11 - ETA: 0:17:46\n",
      "  Train Loss: 0.030662, Train MSE: 215.638896\n",
      "  Val Loss: 0.014284, Val MSE: 61.674171, Val RMSLE: 0.119515\n",
      "  Learning rate: 0.001000\n",
      "  Validation loss change: ↓ 21.38%\n",
      "  ✓ New best validation loss: 0.014284 (improved by 21.38%)\n",
      "  ✓ New best validation loss: 0.011543 (improved by 19.19%)\n",
      "  ✓ New best validation loss: 0.009641 (improved by 16.48%)\n",
      "  ✓ New best validation loss: 0.008267 (improved by 14.25%)\n",
      "  ✓ New best validation loss: 0.007318 (improved by 11.48%)\n",
      "Epoch 11/100 - 0:00:10 - ETA: 0:16:41\n",
      "  Train Loss: 0.019612, Train MSE: 175.338088\n",
      "  Val Loss: 0.006555, Val MSE: 37.587170, Val RMSLE: 0.080964\n",
      "  Learning rate: 0.001000\n",
      "  Validation loss change: ↓ 10.42%\n",
      "  ✓ New best validation loss: 0.006555 (improved by 10.42%)\n",
      "  ✓ New best validation loss: 0.006101 (improved by 6.93%)\n",
      "  ✓ New best validation loss: 0.005740 (improved by 5.92%)\n",
      "  ✓ New best validation loss: 0.005377 (improved by 6.31%)\n",
      "  ✓ New best validation loss: 0.005184 (improved by 3.59%)\n",
      "Epoch 16/100 - 0:00:10 - ETA: 0:15:40\n",
      "  Train Loss: 0.016839, Train MSE: 158.027286\n",
      "  Val Loss: 0.005023, Val MSE: 28.446867, Val RMSLE: 0.070872\n",
      "  Learning rate: 0.001000\n",
      "  Validation loss change: ↓ 3.11%\n",
      "  ✓ New best validation loss: 0.005023 (improved by 3.11%)\n",
      "  ✓ New best validation loss: 0.004991 (improved by 0.63%)\n",
      "  ✓ New best validation loss: 0.004787 (improved by 4.09%)\n",
      "  ✓ New best validation loss: 0.004599 (improved by 3.93%)\n",
      "  ✓ New best validation loss: 0.004527 (improved by 1.57%)\n",
      "Epoch 21/100 - 0:00:11 - ETA: 0:14:42\n",
      "  Train Loss: 0.015641, Train MSE: 147.339212\n",
      "  Val Loss: 0.004406, Val MSE: 22.669346, Val RMSLE: 0.066381\n",
      "  Learning rate: 0.001000\n",
      "  Validation loss change: ↓ 2.66%\n",
      "  ✓ New best validation loss: 0.004406 (improved by 2.66%)\n",
      "  ✗ No improvement for 1/20 epochs. Best: 0.004406\n",
      "  ✓ New best validation loss: 0.004326 (improved by 1.84%)\n",
      "  ✓ New best validation loss: 0.004228 (improved by 2.25%)\n",
      "  ✓ New best validation loss: 0.004148 (improved by 1.90%)\n",
      "Epoch 26/100 - 0:00:11 - ETA: 0:13:44\n",
      "  Train Loss: 0.014932, Train MSE: 142.282784\n",
      "  Val Loss: 0.004156, Val MSE: 18.836714, Val RMSLE: 0.064469\n",
      "  Learning rate: 0.001000\n",
      "  Validation loss change: ↑ 0.20%\n",
      "  ✗ No improvement for 1/20 epochs. Best: 0.004148\n",
      "  ✓ New best validation loss: 0.004074 (improved by 1.79%)\n",
      "  ✗ No improvement for 1/20 epochs. Best: 0.004074\n",
      "  ✗ No improvement for 2/20 epochs. Best: 0.004074\n",
      "  ✓ New best validation loss: 0.003977 (improved by 2.37%)\n",
      "Epoch 31/100 - 0:00:11 - ETA: 0:12:48\n",
      "  Train Loss: 0.014411, Train MSE: 138.121867\n",
      "  Val Loss: 0.003986, Val MSE: 15.959940, Val RMSLE: 0.063137\n",
      "  Learning rate: 0.001000\n",
      "  Validation loss change: ↑ 0.24%\n",
      "  ✗ No improvement for 1/20 epochs. Best: 0.003977\n",
      "  ✗ No improvement for 2/20 epochs. Best: 0.003977\n",
      "  ✗ No improvement for 3/20 epochs. Best: 0.003977\n",
      "  ✓ New best validation loss: 0.003873 (improved by 2.61%)\n",
      "  ✗ No improvement for 1/20 epochs. Best: 0.003873\n",
      "Epoch 36/100 - 0:00:11 - ETA: 0:11:51\n",
      "  Train Loss: 0.013910, Train MSE: 136.183307\n",
      "  Val Loss: 0.003956, Val MSE: 14.799253, Val RMSLE: 0.062895\n",
      "  Learning rate: 0.001000\n",
      "  Validation loss change: ↑ 0.84%\n",
      "  ✗ No improvement for 2/20 epochs. Best: 0.003873\n",
      "  ✗ No improvement for 3/20 epochs. Best: 0.003873\n",
      "  ✗ No improvement for 4/20 epochs. Best: 0.003873\n",
      "  ✗ No improvement for 5/20 epochs. Best: 0.003873\n",
      "  → Reducing learning rate: 0.001000 → 0.000200\n",
      "  ✓ New best validation loss: 0.003835 (improved by 0.97%)\n",
      "Epoch 41/100 - 0:00:10 - ETA: 0:10:55\n",
      "  Train Loss: 0.013364, Train MSE: 132.503180\n",
      "  Val Loss: 0.003872, Val MSE: 14.461588, Val RMSLE: 0.062223\n",
      "  Learning rate: 0.000200\n",
      "  Validation loss change: ↑ 0.95%\n",
      "  ✗ No improvement for 1/20 epochs. Best: 0.003835\n",
      "  ✗ No improvement for 2/20 epochs. Best: 0.003835\n",
      "  ✗ No improvement for 3/20 epochs. Best: 0.003835\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[19]\u001B[39m\u001B[32m, line 15\u001B[39m\n\u001B[32m     13\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, params \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(hyperparameter_combinations):\n\u001B[32m     14\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTraining model \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi+\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(hyperparameter_combinations)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m     model, rmsle_val, history = \u001B[43mevaluate_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     16\u001B[39m     results.append({\n\u001B[32m     17\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m'\u001B[39m: model,\n\u001B[32m     18\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mparams\u001B[39m\u001B[33m'\u001B[39m: params,\n\u001B[32m     19\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mrmsle\u001B[39m\u001B[33m'\u001B[39m: rmsle_val,\n\u001B[32m     20\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mhistory\u001B[39m\u001B[33m'\u001B[39m: history\n\u001B[32m     21\u001B[39m     })\n\u001B[32m     22\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mRMSLE: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrmsle_val\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 15\u001B[39m, in \u001B[36mevaluate_model\u001B[39m\u001B[34m(hidden_layers, activation, learning_rate, dropout_rate, l1_reg, l2_reg, batch_size)\u001B[39m\n\u001B[32m      4\u001B[39m model, optimizer = create_model(\n\u001B[32m      5\u001B[39m     input_dim=input_dim,\n\u001B[32m      6\u001B[39m     hidden_layers=hidden_layers,\n\u001B[32m   (...)\u001B[39m\u001B[32m     11\u001B[39m     l2_reg=l2_reg\n\u001B[32m     12\u001B[39m )\n\u001B[32m     14\u001B[39m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m model, history = \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX_train_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_tensor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     18\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX_val_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val_tensor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     21\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpatience\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m20\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     22\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfactor\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     23\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmin_lr\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.0001\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     24\u001B[39m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Set to 2 for more detailed output\u001B[39;49;00m\n\u001B[32m     25\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     27\u001B[39m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n\u001B[32m     28\u001B[39m model.eval()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 107\u001B[39m, in \u001B[36mtrain_model\u001B[39m\u001B[34m(model, optimizer, X_train, y_train, X_val, y_val, epochs, batch_size, patience, factor, min_lr, verbose)\u001B[39m\n\u001B[32m    105\u001B[39m optimizer.zero_grad()\n\u001B[32m    106\u001B[39m loss.backward()\n\u001B[32m--> \u001B[39m\u001B[32m107\u001B[39m \u001B[43moptimizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    109\u001B[39m \u001B[38;5;66;03m# Accumulate batch loss\u001B[39;00m\n\u001B[32m    110\u001B[39m train_loss += loss.item() * X_batch.size(\u001B[32m0\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/calorie_kaggle/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:485\u001B[39m, in \u001B[36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    480\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    481\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m    482\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    483\u001B[39m             )\n\u001B[32m--> \u001B[39m\u001B[32m485\u001B[39m out = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    486\u001B[39m \u001B[38;5;28mself\u001B[39m._optimizer_step_code()\n\u001B[32m    488\u001B[39m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/calorie_kaggle/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:79\u001B[39m, in \u001B[36m_use_grad_for_differentiable.<locals>._use_grad\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     77\u001B[39m     torch.set_grad_enabled(\u001B[38;5;28mself\u001B[39m.defaults[\u001B[33m\"\u001B[39m\u001B[33mdifferentiable\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m     78\u001B[39m     torch._dynamo.graph_break()\n\u001B[32m---> \u001B[39m\u001B[32m79\u001B[39m     ret = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     80\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m     81\u001B[39m     torch._dynamo.graph_break()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/calorie_kaggle/.venv/lib/python3.13/site-packages/torch/optim/adam.py:246\u001B[39m, in \u001B[36mAdam.step\u001B[39m\u001B[34m(self, closure)\u001B[39m\n\u001B[32m    234\u001B[39m     beta1, beta2 = group[\u001B[33m\"\u001B[39m\u001B[33mbetas\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    236\u001B[39m     has_complex = \u001B[38;5;28mself\u001B[39m._init_group(\n\u001B[32m    237\u001B[39m         group,\n\u001B[32m    238\u001B[39m         params_with_grad,\n\u001B[32m   (...)\u001B[39m\u001B[32m    243\u001B[39m         state_steps,\n\u001B[32m    244\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m246\u001B[39m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    247\u001B[39m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    248\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    249\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    250\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    251\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    252\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    253\u001B[39m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mamsgrad\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    254\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    255\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    256\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    257\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlr\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    258\u001B[39m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mweight_decay\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    259\u001B[39m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43meps\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    260\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmaximize\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    261\u001B[39m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mforeach\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    262\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcapturable\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    263\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdifferentiable\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    264\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfused\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    265\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mgrad_scale\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    266\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfound_inf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    267\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdecoupled_weight_decay\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    268\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    270\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/calorie_kaggle/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:147\u001B[39m, in \u001B[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    145\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m disabled_func(*args, **kwargs)\n\u001B[32m    146\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m147\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/calorie_kaggle/.venv/lib/python3.13/site-packages/torch/optim/adam.py:933\u001B[39m, in \u001B[36madam\u001B[39m\u001B[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[39m\n\u001B[32m    930\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    931\u001B[39m     func = _single_tensor_adam\n\u001B[32m--> \u001B[39m\u001B[32m933\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    934\u001B[39m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    935\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    936\u001B[39m \u001B[43m    \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    937\u001B[39m \u001B[43m    \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    938\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    939\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    940\u001B[39m \u001B[43m    \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[43m=\u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    941\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    942\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    943\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    944\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    945\u001B[39m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    946\u001B[39m \u001B[43m    \u001B[49m\u001B[43meps\u001B[49m\u001B[43m=\u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    947\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    948\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    949\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    950\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    951\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    952\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    953\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/calorie_kaggle/.venv/lib/python3.13/site-packages/torch/optim/adam.py:525\u001B[39m, in \u001B[36m_single_tensor_adam\u001B[39m\u001B[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001B[39m\n\u001B[32m    523\u001B[39m         denom = (max_exp_avg_sqs[i].sqrt() / bias_correction2_sqrt).add_(eps)\n\u001B[32m    524\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m525\u001B[39m         denom = \u001B[43m(\u001B[49m\u001B[43mexp_avg_sq\u001B[49m\u001B[43m.\u001B[49m\u001B[43msqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m/\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias_correction2_sqrt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43madd_\u001B[49m\u001B[43m(\u001B[49m\u001B[43meps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    527\u001B[39m     param.addcdiv_(exp_avg, denom, value=-step_size)\n\u001B[32m    529\u001B[39m \u001B[38;5;66;03m# Lastly, switch back to complex view\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get the best model\n",
    "best_model = best_result['model']\n",
    "\n",
    "# Make predictions on the validation set\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_best = best_model(X_val_tensor).cpu().numpy().flatten()\n",
    "\n",
    "y_pred_best = np.maximum(y_pred_best, 0)  # Ensure predictions are positive\n",
    "\n",
    "# Calculate metrics\n",
    "rmsle_score_best = rmsle(y_val, y_pred_best)\n",
    "rmse_score_best = np.sqrt(mean_squared_error(y_val, y_pred_best))\n",
    "r2_best = r2_score(y_val, y_pred_best)\n",
    "\n",
    "print(f\"Best Neural Network Model Performance:\")\n",
    "print(f\"RMSLE: {rmsle_score_best:.4f}\")\n",
    "print(f\"RMSE: {rmse_score_best:.4f}\")\n",
    "print(f\"R² Score: {r2_best:.4f}\")\n",
    "\n",
    "# Compare with the basic model\n",
    "print(\"\\nImprovement over basic model:\")\n",
    "print(f\"RMSLE improvement: {rmsle_score - rmsle_score_best:.4f} ({(rmsle_score - rmsle_score_best) / rmsle_score * 100:.2f}%)\")\n"
   ],
   "id": "7f59ebbd8df83ac9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Visualize Predictions vs Actual Values\n",
   "id": "62773adef252ad65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a dataframe with actual and predicted values\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': y_val,\n",
    "    'Predicted': y_pred_best\n",
    "})\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(results_df['Actual'], results_df['Predicted'], alpha=0.5)\n",
    "plt.plot([min(y_val), max(y_val)], [min(y_val), max(y_val)], 'r--')\n",
    "plt.xlabel('Actual Calories')\n",
    "plt.ylabel('Predicted Calories')\n",
    "plt.title('Actual vs Predicted Calories')\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals\n",
    "results_df['Residuals'] = results_df['Actual'] - results_df['Predicted']\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(results_df['Residuals'], kde=True)\n",
    "plt.axvline(x=0, color='r', linestyle='--')\n",
    "plt.title('Residuals Distribution')\n",
    "plt.xlabel('Residuals')\n",
    "plt.show()\n"
   ],
   "id": "596b6d006a9b4ca1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. Make Predictions on Test Data\n",
   "id": "c84dbc89a639ef2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T00:54:14.782377Z",
     "start_time": "2025-05-09T00:54:14.740090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert test data to PyTorch tensor\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "\n",
    "# Make predictions on the test set\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = best_model(X_test_tensor).cpu().numpy().flatten()\n",
    "\n",
    "# Ensure predictions are positive\n",
    "test_predictions = np.maximum(test_predictions, 0)\n",
    "\n",
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'Calories': test_predictions\n",
    "})\n",
    "\n",
    "# Display the first few rows of the submission file\n",
    "submission.head()\n"
   ],
   "id": "9c563e611df2dd8e",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[16]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      2\u001B[39m X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# Make predictions on the test set\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m \u001B[43mbest_model\u001B[49m.eval()\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m      7\u001B[39m     test_predictions = best_model(X_test_tensor).cpu().numpy().flatten()\n",
      "\u001B[31mNameError\u001B[39m: name 'best_model' is not defined"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save the submission file\n",
    "submission.to_csv('neural_network_submission.csv', index=False)\n",
    "print(\"Submission file saved successfully!\")\n"
   ],
   "id": "d6ce570be7f85109"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9. Feature Importance Analysis (Permutation Importance)\n",
   "id": "ef81378a1dc8e371"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Since neural networks don't provide feature importance directly,\n",
    "# we can use permutation importance to estimate feature importance\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Create a wrapper for the PyTorch model to use with scikit-learn\n",
    "class PyTorchRegressor:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Convert numpy array to PyTorch tensor\n",
    "        X_tensor = torch.FloatTensor(X).to(self.device)\n",
    "\n",
    "        # Make predictions\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(X_tensor).cpu().numpy().flatten()\n",
    "\n",
    "        return predictions\n",
    "\n",
    "# Create a wrapper for our best model\n",
    "torch_wrapper = PyTorchRegressor(best_model, device)\n",
    "\n",
    "# Calculate permutation importance\n",
    "result = permutation_importance(\n",
    "    torch_wrapper, X_val_scaled, y_val,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Create a dataframe with feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': result.importances_mean\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "plt.title('Feature Importance (Permutation Importance)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "2acefe3755d57a0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "In this notebook, we built a Neural Network model to predict calories burned during workouts. We performed the following steps:\n",
    "\n",
    "1. Loaded and explored the dataset\n",
    "2. Performed feature engineering to create new features that might improve model performance\n",
    "3. Prepared the data for modeling, including scaling the features\n",
    "4. Built a basic neural network model as a baseline\n",
    "5. Tuned the hyperparameters of the model, including network architecture, activation functions, and regularization\n",
    "6. Trained an optimized neural network model with the best hyperparameters\n",
    "7. Evaluated the model's performance using RMSLE (Root Mean Squared Logarithmic Error)\n",
    "8. Analyzed feature importance using permutation importance\n",
    "9. Generated predictions for the test set and created a submission file\n",
    "\n",
    "The optimized neural network model showed good performance on the validation set, with an improvement over the baseline model. The most important features for predicting calories burned were identified through permutation importance analysis.\n"
   ],
   "id": "d9a979c437987ead"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
